[Вопросы для собеседования](README.md)

# Базы данных
+ [Что такое _«база данных»_?](#Что-такое-база-данных)
+ [Что такое _«система управления базами данных»_?](#Что-такое-система-управления-базами-данных)
+ [Что такое _«реляционная модель данных»_?](#Что-такое-реляционная-модель-данных)
+ [Дайте определение терминам _«простой»_, _«составной» (composite)_, _«потенциальный» (candidate)_ и _«альтернативный» (alternate)_ ключ.](#Дайте-определение-терминам-простой-составной-composite-потенциальный-candidate-и-альтернативный-alternate-ключ)
+ [Что такое _«первичный ключ» (primary key)_? Каковы критерии его выбора?](#Что-такое-первичный-ключ-primary-key-Каковы-критерии-его-выбора)
+ [Что такое _«внешний ключ» (foreign key)_?](#Что-такое-внешний-ключ-foreign-key)
+ [Что такое _«нормализация»_?](#Что-такое-нормализация)
+ [Какие существуют нормальные формы?](#Какие-существуют-нормальные-формы)
+ [Что такое _«денормализация»_? Для чего она применяется?](#Что-такое-денормализация-Для-чего-она-применяется)
+ [Какие существуют типы связей в базе данных? Приведите примеры.](#Какие-существуют-типы-связей-в-базе-данных-Приведите-примеры)
+ [Что такое _«индексы»_? Для чего их используют? В чём заключаются их преимущества и недостатки?](#Что-такое-индексы-Для-чего-их-используют-В-чём-заключаются-их-преимущества-и-недостатки)
+ [Какие типы индексов существуют?](#Какие-типы-индексов-существуют)
+ [В чем отличие между кластерными и некластерными индексами?](#В-чем-отличие-между-кластерными-и-некластерными-индексами)
+ [Имеет ли смысл индексировать данные, имеющие небольшое количество возможных значений?](#Имеет-ли-смысл-индексировать-данные-имеющие-небольшое-количество-возможных-значений)
+ [Когда полное сканирование набора данных выгоднее доступа по индексу?](#Когда-полное-сканирование-набора-данных-выгоднее-доступа-по-индексу)
+ [Что такое _«транзакция»_?](#Что-такое-транзакция)
+ [Назовите основные свойства транзакции.](#Назовите-основные-свойства-транзакции)
+ [Какие существуют уровни изолированности транзакций?](#Какие-существуют-уровни-изолированности-транзакций)
+ [Какие проблемы могут возникать при параллельном доступе с использованием транзакций?](#Какие-проблемы-могут-возникать-при-параллельном-доступе-с-использованием-транзакций)

## Что такое _«база данных»_?
__База данных__ — организованный и адаптированный для обработки вычислительной системой набор информации.

[к оглавлению](#Базы-данных)

## Что такое _«система управления базами данных»_?
__Система управления базами данных (СУБД)__ - набор средств общего или специального назначения, обеспечивающий создание, доступ к материалам и управление базой данных.

Основные функции СУБД:

+ управление данными
+ журнализация изменений данных
+ резервное копирование и восстановление данных;
+ поддержка языка определения данных и манипулирования ими.

[к оглавлению](#Базы-данных)

## Что такое _«реляционная модель данных»_?
__Реляционная модель данных__ — это логическая модель данных и прикладная теория построения реляционных баз данных.

Реляционная модель данных включает в себя следующие компоненты:

+ _Структурный аспект_ — данные представляют собой набор отношений.
+ _Аспект целостности_ — отношения отвечают определенным условиям целостности: уровня домена (типа данных), уровня отношения и уровня базы данных.
+ _Аспект обработки (манипулирования)_ — поддержка операторов манипулирования отношениями (реляционная алгебра, реляционное исчисление).
+ _Нормальная форма_ - свойство отношения в реляционной модели данных, характеризующее его с точки зрения избыточности и определённое как совокупность требований, которым должно удовлетворять отношение.

[к оглавлению](#Базы-данных)

## Дайте определение терминам _«простой»_, _«составной» (composite)_, _«потенциальный» (candidate)_ и _«альтернативный» (alternate)_ ключ.
__Простой ключ__ состоит из одного атрибута (поля). __Составной__ - из двух и более.

__Потенциальный ключ__ - простой или составной ключ, который уникально идентифицирует каждую запись набора данных. При этом потенциальный ключ должен обладать критерием неизбыточности: при удалении любого из полей набор полей перестает уникально идентифицировать запись.

Из множества всех потенциальных ключей набора данных выбирают первичный ключ, все остальные ключи называют __альтернативными__.

[к оглавлению](#Базы-данных)

## Что такое _«первичный ключ» (primary key)_? Каковы критерии его выбора?
__Первичный ключ (primary key)__ в реляционной модели данных один из _потенциальных ключей_ отношения, выбранный в качестве основного ключа (ключа по умолчанию).

Если в отношении имеется единственный потенциальный ключ, он является и первичным ключом. Если потенциальных ключей несколько, один из них выбирается в качестве первичного, а другие называют _«альтернативными»_.

В качестве первичного обычно выбирается тот из потенциальных ключей, который наиболее удобен. Поэтому в качестве первичного ключа, как правило, выбирают тот, который имеет наименьший размер (физического хранения) и/или включает наименьшее количество атрибутов. Другой критерий выбора первичного ключа — сохранение его уникальности со временем. Поэтому в качестве первичного ключа стараются выбирать такой потенциальный ключ, который с наибольшей вероятностью никогда не утратит уникальность.

[к оглавлению](#Базы-данных)

## Что такое _«внешний ключ» (foreign key)_?
__Внешний ключ (foreign key)__ — подмножество атрибутов некоторого отношения A, значения которых должны совпадать со значениями некоторого потенциального ключа некоторого отношения B.

[к оглавлению](#Базы-данных)

## Что такое _«нормализация»_?
_Нормализация_ - это процесс преобразования отношений базы данных к виду, отвечающему нормальным формам (пошаговый, обратимый процесс замены исходной схемы другой схемой, в которой наборы данных имеют более простую и логичную структуру).

Нормализация предназначена для приведения структуры базы данных к виду, обеспечивающему минимальную логическую избыточность, и не имеет целью уменьшение или увеличение производительности работы или же уменьшение или увеличение физического объёма базы данных. Конечной целью нормализации является уменьшение потенциальной противоречивости хранимой в базе данных информации.

[к оглавлению](#Базы-данных)

## Какие существуют нормальные формы?
__Первая нормальная форма (1NF)__ - Отношение находится в 1NF, если значения всех его атрибутов атомарны (неделимы). 

__Вторая нормальная форма (2NF)__ - Отношение находится в 2NF, если оно находится в 1NF, и при этом все неключевые атрибуты зависят только от ключа целиком, а не от какой-то его части.

__Третья нормальная форма (3NF)__ - Отношение находится в 3NF, если оно находится в 2NF и все неключевые атрибуты не зависят друг от друга.

__Четвёртая нормальная форма (4NF)__ - Отношение находится в 4NF , если оно находится в 3NF и если в нем не содержатся независимые группы атрибутов, между которыми существует отношение «многие-ко-многим».

__Пятая нормальная форма (5NF)__ - Отношение находится в 5NF, когда каждая нетривиальная зависимость соединения в ней определяется потенциальным ключом (ключами) этого отношения.

__Шестая нормальная форма (6NF)__ - Отношение находится в 6NF, когда она удовлетворяет всем нетривиальным зависимостям соединения, т.е. когда она неприводима, то есть не может быть подвергнута дальнейшей декомпозиции без потерь. Каждая переменная отношения, которая находится в 6NF, также находится и в 5NF. Введена как обобщение пятой нормальной формы для хронологической базы данных.

__Нормальная форма Бойса-Кодда, усиленная 3 нормальная форма (BCNF)__ - Отношение находится в BCNF, когда каждая её нетривиальная и неприводимая слева функциональная зависимость имеет в качестве своего детерминанта некоторый потенциальный ключ.

__Доменно-ключевая нормальная форма (DKNF)__ -  Отношение находится в DKNF, когда каждое наложенное на неё ограничение является логическим следствием ограничений доменов и ограничений ключей, наложенных на данное отношение.

[к оглавлению](#Базы-данных)

## Что такое _«денормализация»_? Для чего она применяется?
__Денормализация базы данных__ — это процесс осознанного приведения базы данных к виду, в котором она не будет соответствовать правилам нормализации. Обычно это необходимо для повышения производительности и скорости извлечения данных, за счет увеличения избыточности данных.

[к оглавлению](#Базы-данных)

## Какие существуют типы связей в базе данных? Приведите примеры.
+ __Один к одному__ - любому значению атрибута А соответствует только одно значение атрибута В, и наоборот.

>Каждый университет гарантированно имеет 1-го ректора: _1 университет → 1 ректор_.

+ __Один ко многим__ - любому значению атрибута А соответствует 0, 1 или несколько значений атрибута В.

>В каждом университете есть несколько факультетов: _1 университет → много факультетов_.

+ __Многие ко многим__ - любому значению атрибута А соответствует 0, 1 или несколько значений атрибута В, и любому значению атрибута В соответствует 0, 1 или несколько значение атрибута А.

>1 профессор может преподавать на нескольких факультетах, в то же время на 1-ом факультете может преподавать несколько профессоров: _Несколько профессоров ↔ Несколько факультетов_.

[к оглавлению](#Базы-данных)

## Что такое _«индексы»_? Для чего их используют? В чём заключаются их преимущества и недостатки?
__Индекс (index)__ — объект базы данных, создаваемый с целью повышения производительности выборки данных. 

Наборы данных могут иметь большое количество записей, которые хранятся в произвольном порядке, и их поиск по заданному критерию путём последовательного просмотра набора данных запись за записью может занимать много времени. Индекс формируется из значений одного или нескольких полей и указателей на соответствующие записи набора данных, - таким образом, достигается значительный прирост скорости выборки из этих данных.

Преимущества

+ ускорение поиска и сортировки по определенному полю или набору полей.
+ обеспечение уникальности данных.

Недостатки 

+ требование дополнительного места на диске и в оперативной памяти и чем больше/длиннее ключ, тем больше размер индекса.
+ замедление операций вставки, обновления и удаления записей, поскольку при этом приходится обновлять сами индексы.

Индексы предпочтительней для:

+ Поля-счетчика, чтобы в том числе избежать и повторения значений в этом поле;
+ Поля, по которому проводится сортировка данных;
+ Полей, по которым часто проводится соединение наборов данных. Поскольку в этом случае данные располагаются в порядке возрастания индекса и соединение происходит значительно быстрее;
+ Поля, которое объявлено первичным ключом (primary key);
+ Поля, в котором данные выбираются из некоторого диапазона. В этом случае как только будет найдена первая запись с нужным значением, все последующие значения будут расположены рядом.
 
Использование индексов нецелесообразно для:

+ Полей, которые редко используются в запросах;
+ Полей, которые содержат всего два или три значения, например: _мужской_,  _женский пол_ или значения _«да»_, _«нет»_.

[к оглавлению](#Базы-данных)

## Какие типы индексов существуют?

__По порядку сортировки__
+ _упорядоченные_ — индексы, в которых элементы упорядочены;
+ _возрастающие_;
+ _убывающие_;
+ _неупорядоченные_ — индексы, в которых элементы неупорядочены.

__По источнику данных__
+ _индексы по представлению (view)_;
+ _индексы по выражениям_.

__По воздействию на источник данных__
+ _кластерный индекс_ - при определении в наборе данных физическое расположение данных перестраивается в соответствии со структурой индекса. Логическая структура набора данных в этом случае представляет собой скорее словарь, чем индекс. Данные в словаре физически упорядочены, например по алфавиту. Кластерные индексы могут дать существенное увеличение производительности поиска данных даже по сравнению с обычными индексами. Увеличение производительности особенно заметно при работе с последовательными данными.
+ _некластерный индекс_ — наиболее типичные представители семейства индексов. В отличие от кластерных, они не перестраивают физическую структуру набора данных, а лишь организуют ссылки на соответствующие записи. Для идентификации нужной записи в наборе данных некластерный индекс организует специальные указатели, включающие в себя: информацию об идентификационном номере файла, в котором хранится запись; идентификационный номер страницы соответствующих данных; номер искомой записи на соответствующей странице; содержимое столбца.

__По структуре__
+ _B*-деревья_;
+ _B+-деревья_;
+ _B-деревья_;
+ _Хэши_.

__По количественному составу__
+ _простой индекс (индекс с одним ключом)_ — строится по одному полю;
+ _составной (многоключевой, композитный) индекс_ — строится по нескольким полям при этом важен порядок их следования;
+ _индекс с включенными столбцами_ — некластеризованный индекс, дополнительно содержащий кроме ключевых столбцов еще и неключевые;
+ _главный индекс (индекс по первичному ключу)_ — это тот индексный ключ, под управлением которого в данный момент находится набор данных. Набор данных не может быть отсортирован по нескольким индексным ключам одновременно. Хотя, если один и тот же набор данных открыт одновременно в нескольких рабочих областях, то у каждой копии набора данных может быть назначен свой главный индекс.

__По характеристике содержимого__
+ _уникальный индекс_ состоит из множества уникальных значений поля;
+ _плотный индекс_ (NoSQL) — индекс, при котором, каждому документу в индексируемой коллекции соответствует запись в индексе, даже если в документе нет индексируемого поля.
+ _разреженный индекс_ (NoSQL) — тот, в котором представлены только те документы, для которых индексируемый ключ имеет какое-то определённое значение (существует).
+ _пространственный индекс_ — оптимизирован для описания географического местоположения. Представляет из себя многоключевой индекс состоящий из широты и долготы.
+ _составной пространственный индекс_ — индекс, включающий в себя кроме широты и долготы ещё какие-либо мета-данные (например теги). Но географические координаты должны стоять на первом месте.
+ _полнотекстовый (инвертированный) индекс_ — словарь, в котором перечислены все слова и указано, в каких местах они встречаются. При наличии такого индекса достаточно осуществить поиск нужных слов в нём и тогда сразу же будет получен список документов, в которых они встречаются.
+ _хэш-индекс_ предполагает хранение не самих значений, а их хэшей, благодаря чему уменьшается размер (а, соответственно, и увеличивается скорость их обработки) индексов из больших полей. Таким образом, при запросах с использованием хэш-индексов, сравниваться будут не искомое со значения поля, а хэш от искомого значения с хэшами полей.
Из-за нелинейнойсти хэш-функций данный индекс нельзя сортировать по значению, что приводит к невозможности использования в сравнениях больше/меньше и «is null». Кроме того, так как хэши не уникальны, то для совпадающих хэшей применяются методы разрешения коллизий.
+ _битовый индекс (bitmap index)_ — метод битовых индексов заключается в создании отдельных битовых карт (последовательностей 0 и 1) для каждого возможного значения столбца, где каждому биту соответствует запись с индексируемым значением, а его значение равное 1 означает, что запись, соответствующая позиции бита содержит индексируемое значение для данного столбца или свойства.
+ _обратный индекс (reverse index)_ — B-tree индекс, но с реверсированным ключом, используемый в основном для монотонно возрастающих значений (например, автоинкрементный идентификатор) в OLTP системах с целью снятия конкуренции за последний листовой блок индекса, т.к. благодаря переворачиванию значения две соседние записи индекса попадают в разные блоки индекса. Он не может использоваться для диапазонного поиска.
+ _функциональный индекс, индекс по вычисляемому полю (function-based index)_ — индекс, ключи которого хранят результат пользовательских функций. Функциональные индексы часто строятся для полей, значения которых проходят предварительную обработку перед сравнением в команде SQL. Например, при сравнении строковых данных без учета регистра символов часто используется функция UPPER. Кроме того, функциональный индекс может помочь реализовать любой другой отсутствующий тип индексов данной СУБД.
+ _первичный индекс_ — уникальный индекс по полю первичного ключа.
+ _вторичный индекс_ — индекс по другим полям (кроме поля первичного ключа).
+ _XML-индекс_ — вырезанное материализованное представление больших двоичных XML-объектов (BLOB) в столбце с типом данных xml.

__По механизму обновления__
+ _полностью перестраиваемый_ — при добавлении элемента заново перестраивается весь индекс.
+ _пополняемый (балансируемый)_ — при добавлении элементов индекс перестраивается частично (например, одна из ветви) и периодически балансируется.

__По покрытию индексируемого содержимого__
+ _полностью покрывающий (полный) индекс_ — покрывает всё содержимое индексируемого объекта.
+ _частичный индекс (partial index)_ — это индекс, построенный на части набора данных, удовлетворяющей определенному условию самого индекса. Данный индекс создан для уменьшения размера индекса.
+ _инкрементный (delta) индекс_ — индексируется малая часть данных(дельта), как правило, по истечении определённого времени. Используется при интенсивной записи. Например, полный индекс перестраивается раз в сутки, а дельта-индекс строится каждый час. По сути это частичный индекс по временной метке.
+ _индекс реального времени (real-time index)_ — особый вид инкрементного индекса, характеризующийся высокой скоростью построения. Предназначен для часто меняющихся данных.

__Индексы в кластерных системах__
+ _глобальный индекс_ — индекс по всему содержимому всех сегментов БД (shard).
+ _сегментный индекс_ — глобальный индекс по полю-сегментируемому ключу (shard key). Используется для быстрого определения сегмента, на котором хранятся данные в процессе маршрутизации запроса в кластере БД.
+ _локальный индекс_ —  индекс по содержимому только одного сегмента БД.

[к оглавлению](#Базы-данных)

## В чем отличие между кластерными и некластерными индексами?
Некластерные индексы - данные физически расположены в произвольном порядке, но логически упорядочены согласно индексу. Такой тип индексов подходит для часто изменяемого набора данных.

При кластерном индексировании данные физически упорядочены, что серьезно повышает скорость выборок данных (но только в случае последовательного доступа к данным). Для одного набора данных может быть создан только один кластерный индекс.

[к оглавлению](#Базы-данных)

## Имеет ли смысл индексировать данные, имеющие небольшое количество возможных значений?
Примерное правило, которым можно руководствоваться при создании индекса - если объем информации (в байтах) НЕ удовлетворяющей условию выборки меньше, чем размер индекса (в байтах) по данному условию выборки, то в общем случае оптимизация приведет к замедлению выборки.

[к оглавлению](#Базы-данных)

## Когда полное сканирование набора данных выгоднее доступа по индексу?
Полное сканирование производится многоблочным чтением. Сканирование по индексу - одноблочным. Также, при доступе по индексу сначала идет сканирование самого индекса, а затем чтение блоков из набора данных. Число блоков, которые надо при этом прочитать из набора зависит от фактора кластеризации. Если суммарная стоимость всех необходимых одноблочных чтений больше стоимости полного сканирования многоблочным чтением, то полное сканирование выгоднее, и оно выбирается оптимизатором.

Таким образом, полное сканирование выбирается при слабой селективности предикатов запроса и/или слабой кластеризации данных, либо в случае очень маленьких наборов данных.

[к оглавлению](#Базы-данных)

## Что такое _«транзакция»_?
__Транзакция__ - это воздействие на базу данных, переводящее её из одного целостного состояния в другое и выражаемое в изменении данных, хранящихся в базе данных.

[к оглавлению](#Базы-данных)

## Назовите основные свойства транзакции.
__Атомарность (atomicity)__ гарантирует, что никакая транзакция не будет зафиксирована в системе частично. Будут либо выполнены все её подоперации, либо не выполнено ни одной. 

__Согласованность (consistency)__. Транзакция, достигающая своего нормального завершения и, тем самым, фиксирующая свои результаты, сохраняет согласованность базы данных.

__Изолированность (isolation)__. Во время выполнения транзакции параллельные транзакции не должны оказывать влияние на её результат.

__Долговечность (durability)__. Независимо от проблем на нижних уровнях (к примеру, обесточивание системы или сбои в оборудовании) изменения, сделанные успешно завершённой транзакцией, должны остаться сохранёнными после возвращения системы в работу.

[к оглавлению](#Базы-данных)

## Какие существуют уровни изолированности транзакций?
В порядке увеличения изолированности транзакций и, соответственно, надёжности работы с данными:

+ __Чтение неподтверждённых данных (грязное чтение) (read uncommitted, dirty read)__ — чтение незафиксированных изменений как своей транзакции, так и параллельных транзакций. Нет гарантии, что данные, изменённые другими транзакциями, не будут в любой момент изменены в результате их отката, поэтому такое чтение является потенциальным источником ошибок. Невозможны потерянные изменения, возможны неповторяемое чтение и фантомы.
+ __Чтение подтверждённых данных (read committed)__ — чтение всех изменений своей транзакции и зафиксированных изменений параллельных транзакций. Потерянные изменения и грязное чтение не допускается, возможны неповторяемое чтение и фантомы.
+ __Повторяемость чтения (repeatable read, snapshot)__ — чтение всех изменений своей транзакции, любые изменения, внесённые параллельными транзакциями после начала своей, недоступны. Потерянные изменения, грязное и неповторяемое чтение невозможны, возможны фантомы.
+ __Упорядочиваемость (serializable)__ — результат параллельного выполнения сериализуемой транзакции с другими транзакциями должен быть логически эквивалентен результату их какого-либо последовательного выполнения. Проблемы синхронизации не возникают.

[к оглавлению](#Базы-данных)

## Какие проблемы могут возникать при параллельном доступе с использованием транзакций?
При параллельном выполнении транзакций возможны следующие проблемы:

+ __Потерянное обновление (lost update)__ — при одновременном изменении одного блока данных разными транзакциями одно из изменений теряется;
+ __«Грязное» чтение (dirty read)__ — чтение данных, добавленных или изменённых транзакцией, которая впоследствии не подтвердится (откатится);
+ __Неповторяющееся чтение (non-repeatable read)__ — при повторном чтении в рамках одной транзакции ранее прочитанные данные оказываются изменёнными;
+ __Фантомное чтение (phantom reads)__ — одна транзакция в ходе своего выполнения несколько раз выбирает множество записей по одним и тем же критериям. Другая транзакция в интервалах между этими выборками добавляет или удаляет записи или изменяет столбцы некоторых записей, используемых в критериях выборки первой транзакции, и успешно заканчивается. В результате получится, что одни и те же выборки в первой транзакции дают разные множества записей. 
Предположим, имеется две транзакции, открытые различными приложениями, в которых выполнены следующие SQL-операторы:

| Транзакция 1 |	Транзакция 2 |
|--------------|--------------|
| | SELECT SUM(f2) FROM tbl1; |
| INSERT INTO tbl1 (f1,f2) VALUES (15,20);	| |
| COMMIT;	| |
| | SELECT SUM(f2) FROM tbl1;|

В транзакции 2 выполняется SQL-оператор, использующий все значения поля f2. Затем в транзакции 1 выполняется вставка новой строки, приводящая к тому, что повторное выполнение SQL-оператора в транзакции 2 выдаст другой результат. Такая ситуация называется чтением фантома (фантомным чтением). От неповторяющегося чтения оно отличается тем, что результат повторного обращения к данным изменился не из-за изменения/удаления самих этих данных, а из-за появления новых (фантомных) данных.

[к оглавлению](#Базы-данных)

Нет вопросов — приступаю. Ниже — развёрнутый ответ на русском, ориентированный на собеседование Java-разработчика: что такое индексы в PostgreSQL, зачем нужны, принцип работы, виды, плюсы/минусы, тонкости, примеры запросов/диаграммы и рекомендации по использованию. Я опирался(ась) на официальную документацию PostgreSQL и авторитетные материалы (ссылки после ключевых блоков).  

# Что такое индекс в реляционной БД (простыми словами)
Индекс — это вспомогательная структура данных, которая хранит информацию о значениях в столбцах таблицы и ссылках на строки (tuple/heap). По индексу БД может находить нужные строки быстрее, не сканируя весь стол. Про аналогию: это как алфавитный указатель в книге — вместо того чтобы читать всю книгу, вы смотрите в указатель и переходите на нужную страницу.

> Коротко: индекс ускоряет чтение (SELECT), но замедляет запись (INSERT/UPDATE/DELETE) и занимает место.

# Основные типы индексов в PostgreSQL (какие бывают)
Postgres поддерживает несколько типов индексов; ключевые — и когда их применять:

## Краткая схема-ориентир

Если нужен быстрый поиск по точному совпадению или диапазонам → **B-tree**.

Если нужно индексировать поля с множественными значениями (jsonb, массивы, tsvector) → **GIN**.

Для геометрии, диапазонов, KNN или специальных операторов → **GiST**.

Для специфичных структур (префиксные деревья, квадродерево и т.п.) → **SP-GiST**.

Для огромных append-only таблиц с корреляцией по физическому порядку (время) → **BRIN**.

**Hash** — для очень специализированных случаев точного сравнения; обычно B-tree достаточно.

---

## Теперь перейдём по каждому типу подробно.

### B-tree (balanced tree) — «универсал»

**Что это:** сбалансированное дерево (B-tree) — основной индекс по умолчанию в Postgres.  
**Что быстро делает:** `=`, `<`, `<=`, `>`, `>=`, `BETWEEN`, сортировки (`ORDER BY`) и префиксные поиски по однотипным типам данных. Поддерживает уникальные индексы.

**Где применять:** почти везде, где нужны точечные или диапазонные запросы: id, внешние ключи, даты, числовые поля, строки для `LIKE 'abc%'`.

**Плюсы:**
- Хорошая производительность для одиночных запросов и диапазонов.
- Поддерживает уникальность (PRIMARY KEY).
- Может обслуживать `ORDER BY` без отдельной сортировки, если порядок совпадает с индексом.
- Поддерживает мультиколоночные индексы (leftmost-prefix).

**Минусы:**
- Занимает место; замедляет `INSERT/UPDATE/DELETE` (каждая DML меняет индекс).
- Неэффективен для полей с множеством значений в одной ячейке (jsonb, массивы).
- Для `LIKE '%foo%'` без триграмм будет малоэффективен (ведёт к seq scan).

**Практические советы:**
- Для запроса `WHERE a = X AND b BETWEEN Y AND Z` сделайте составной индекс `(a, b)`.
- Если нужно `ORDER BY created_at DESC LIMIT 20` по пользователю — индекс `(user_id, created_at DESC)` даст быстрый ответ.
- Не создавайте одновременно индекс на `user_id` и ещё один на `(user_id, created_at)`: второй покрывает первый (leftmost).

**Пример SQL:**
```sql
CREATE INDEX idx_orders_user_date ON orders (user_id, created_at DESC);
```

---

### Hash — индекс для точного сравнения

**Что это:** индекс, оптимизированный для `=` (точного сравнения).  
**Что быстро делает:** поиск на точное равенство.

**Где применять:** когда у вас очень частые точечные равенства по большой таблице, и вы уверены, что B-tree не нужен по другим причинам.

**Плюсы:**
- Может быть чуть компактнее/оптимальнее для `=`-поиска в узких случаях.

**Минусы / оговорки:**
- Исторически имел ограничения; обычно B-tree достаточно и надёжнее.
- В большинстве практических сценариев B-tree даёт сравнимую или лучшую производительность и больше возможностей (range, sort и т.д.).

**Пример SQL:**
```sql
CREATE INDEX idx_users_hash_email ON users USING hash (email);
```

**Когда не использовать:** если нужны диапазоны, `ORDER BY` или универсальность — берите B-tree.

---

### GIN (Generalized Inverted Index) — «инвертированный индекс» для множеств

**Что это:** инвертированный индекс — хранит для каждого «термина» (ключа) список строк, где этот термин встречается. Отлично для `jsonb`, массивов, полнотекстового поиска (`tsvector`), и для триграмм.

**Что быстро делает:** поиск по наличию ключа/элемента в структуре: `jsonb @>`, `array @>`, `to_tsvector`, поиск по множественным значениям.

**Где применять:**
- Поиск в `jsonb` (`attributes @> '{"color":["red"]}'`).
- Полнотекстовый поиск по `to_tsvector(title)`.
- Индексация массивов (например, тегов).
- `pg_trgm` (LIKE/ILIKE/сходство) часто использует GIN.

**Плюсы:**
- Очень быстрый для поисков «есть такой ключ/терм?».
- Подходит для полей с множественными значениями.

**Минусы:**
- Тяжёлый при вставках/обновлениях — GIN дороже поддерживать.
- Индекс занимает заметно больше места, чем простой B-tree.
- По умолчанию GIN использует механизм `fastupdate` (накопление записей во вспомогательной структуре), который уменьшает нагрузку, но имеет свои trade-offs.

**Практические советы:**
- Если `jsonb`/массивы часто обновляются для большинства строк — подумайте о денормализации часто-используемых полей в отдельные колонки (B-tree).
- Можно делать partial GIN (`WHERE in_stock = true`), если фильтры узкие.
- Регулярно мониторьте `pg_stat_all_indexes` для `idx_scan` и размер индекса.

**Примеры SQL:**
```sql
-- jsonb
CREATE INDEX idx_product_attr_gin ON product USING gin (attributes);

-- полнотекст
CREATE INDEX idx_docs_tsv ON documents USING gin (to_tsvector('russian', text));
```

---

### GiST (Generalized Search Tree) — обобщённый поисковый индекс

**Что это:** фреймворк для реализации разных типов индексов (R-tree-подобных, kd-tree и т.п.). Часто применяется в геопространственных данных (PostGIS), а также для некоторых типов диапазонов и приближённого поиска.

**Что быстро делает:** быстрый поиск для пространственных запросов (попадание по области), поиск по диапазкам/интервалам, KNN (k-nearest neighbors) при поддержке.

**Где применять:**
- Геоданные (точки, полигоны) — PostGIS использует GiST.
- Индексация `range`-типов (`int4range`, `tsrange`) — быстро ищет пересечения/включения.
- Триграммы с GiST (альтернатива GIN для `pg_trgm`).

**Плюсы:**
- Гибкость — можно реализовать специализированные структуры под свои операторы.
- Поддерживает пространственные и диапазонные операции.

**Минусы:**
- Не всегда такой же быстрый как GIN для вхождений по множественным терминам.
- Сложнее тонко настраивать; поведение зависит от реализации операторов.

**Практические советы:**
- Для геоданных используйте GiST (или rtree-варианты в расширениях) — это стандарт для PostGIS.
- Для `range`-типов GiST очень удобен: `WHERE tsrange && ts` (пересечение) — GiST индекс поможет.

**Пример SQL:**
```sql
-- spatial (PostGIS)
CREATE INDEX idx_geom_gist ON places USING gist (geom);

-- range overlap
CREATE INDEX idx_events_timerange ON events USING gist (tstzrange(start_time, end_time));
```

---

### SP-GiST (Space-Partitioned GiST) — специализированная пространственная структура

**Что это:** разновидность GiST, ориентированная на разделение пространства (tries, quadtrees, radix trees). Подходит для структур, где элементы естественно делятся на непересекающиеся подпространства.

**Где применять:**
- Префиксные поиски (например для IP-адресов, префиксов).
- Очень большое количество маленьких «кластеров» в пространстве — когда стандартный GiST «не укладывается» по структуре.

**Плюсы:**
- Быстрая вставка/поиск при типичных распределениях в таких структурах.
- Можно получить хорошие KNN/lookup-характеристики для особых случаев.

**Минусы:**
- Менее универсален; нужно понимать, что структура подходит именно под ваши данные.
- Меньше известных «готовых» сценариев, чем у GiST/GIN.

**Пример SQL (абстрактно):**
```sql
CREATE INDEX idx_ip_spgist ON sessions USING spgist (client_ip inet_ops);
```

---

### BRIN (Block Range INdexes) — «очень компактный» индекс

**Что это:** индекс не по каждой строке, а по диапазонам блоков на диске. Для каждого блока (или группы блоков) хранится краткая сводка (min/max или другие агрегаты).

**Что быстро делает:** быстро отфильтровывает большие диапазоны страниц, если значения колонки локально упорядочены; идеально для больших time-series и логов.

**Где применять:**
- Очень большие таблицы (миллионы/миллиарды строк), где данные физически упорядочены по колонке (например, вставки по времени).
- Когда нужен грубый предварительный фильтр с минимальным размером индекса.

**Плюсы:**
- Очень компактный по объёму (порядок килобайт вместо гигабайт).
- Очень дешёв в поддержке при вставках — почти не замедляет DML.

**Минусы:**
- Работает хорошо только при корреляции колонка↔физический порядок. Если данные хаотичны — BRIN бесполезен.
- Точность фильтра ниже, чем у B-tree: часто после BRIN идёт seq scan по отобранным блокам.

**Практические советы:**
- Используйте BRIN на логах/таймсериях: `event_time` у логов, если записи append-only.
- Комбинируйте с партиционированием по времени для ещё лучшего эффекта.

**Пример SQL:**
```sql
CREATE INDEX idx_event_time_brin ON event_log USING brin (event_time);
```

---

## Вспомогательные концепции и «вариации» индексов (важно знать)

Эти опции не отдельный «тип», но позволяют гибко применять индексы:

- **Составные (multi-column) индексы:** индекс по нескольким колонкам `(a,b)`; действуют по правилу leftmost-prefix.
- **Expression / functional index:** индекс на выражение, например `lower(email)` — нужен, когда запрос использует функцию.

```sql
CREATE INDEX idx_email_lower ON users (lower(email));
```

- **Partial index:** индекс только на часть строк (`WHERE status = 'open'`) — экономит место и ускоряет редкие кейсы.
- **INCLUDE (covering index):** добавить колонки в индекс как неключевые (для index-only scan), например `INCLUDE (total_amount)`.

```sql
CREATE INDEX idx_orders_user_date_inc ON orders (user_id, created_at DESC) INCLUDE (total_amount);
```

- **Unique index:** обеспечивает уникальность значений. Обычно B-tree.
- **Trigram / pg_trgm:** расширение, даёт быстрый поиск `LIKE '%foo%'` через GIN/GiST и триграммы.

---

## Практическая «шпаргалка» — как выбирать индекс (простая логика)

- Запросы чаще делают `=` и диапазоны (`BETWEEN`, сравнения) → **B-tree**.
- Индекс на `jsonb`, массивы, `tsvector`, теги → **GIN**.
- Пространственные данные или диапазоны (overlaps, contains) → **GiST**.
- Данные упорядочены по времени, таблица огромна → **BRIN** (если корреляция есть).
- Нужен префикс/специфичная структура (IP, префиксы) → **SP-GiST**.
- Очень частые точечные `=` и ничего больше — можно рассмотреть **Hash**, но чаще берут **B-tree**.

---

## Тонкости эксплуатации и обслуживание

- Индексы ускоряют чтение, но замедляют запись — балансируйте.
- Для эффективных index-only scan нужен visibility map → поддерживайте `VACUUM`/`autovacuum`.
- HOT-updates: если вы часто обновляете неиндексируемые колонки, PostgreSQL может делать HOT и не трогать индекс → храните часто меняемые данные отдельно.
- Следите за `pg_stat_all_indexes`, `pg_relation_size` и `pg_total_relation_size` — чтобы видеть использование и размер.
- При сильном bloat используйте `REINDEX` или реструктуризацию. Для BRIN можно `BRIN_REINDEX` и т.п.

---

## Короткие реальные примеры (быстро)

- Логи по времени: BRIN по `event_time`.
- Каталог товаров с JSON-атрибутами: GIN по `attributes jsonb`.
- Страница последних заказов пользователя: `(user_id, created_at DESC)` — B-tree с `INCLUDE(total_amount)` для index-only.
- Геолокация (PostGIS): GiST по `geom`.
- Поиск по описанию (full-text): GIN по `to_tsvector()`.

---

## Что упомянуть на собеседовании (ключевые тезисы)

- Назвать типы (B-tree, GIN, GiST, SP-GiST, BRIN, Hash).
- Объяснить, почему GIN хорош для `jsonb`/массивов, а BRIN — для append-only таблиц.
- Упомянуть leftmost-prefix, index-only scan и visibility map, HOT updates.
- Показать пример `EXPLAIN (ANALYZE)` до/после индекса для практической аргументации.

---



# Как работает индекс (принцип, упрощённая схема)
1. **Структура**: B-tree — дерево уровней: корень → промежуточные узлы → листовые узлы. Листовые узлы содержат ключи и указатели на конкретные строки (CTID — физическое местоположение tuple в heap).  
   Текстовая диаграмма (упрощённо):

```
          [root]
          /    \
   [node A]    [node B]
    /  \         /  \
 [leaf1][leaf2][leaf3][leaf4]
leaf: (key -> CTID, key -> CTID, ...)
```
Индекс btree, он же B-дерево, пригоден для данных, которые можно отсортировать. Иными словами, для типа данных должны быть определены операторы «больше», «больше или равно», «меньше», «меньше или равно» и «равно». Заметьте, что одни и те же данные иногда можно сортировать разными способами, что возвращает нас к концепции семейства операторов.

Как всегда, индексные записи B-дерева упакованы в страницы. В листовых страницах эти записи содержат индексируемые данные (ключи) и ссылки на строки таблицы (TID-ы); во внутренних страницах каждая запись ссылается на дочернюю страницу индекса и содержит минимальное значение ключа в этой странице.

B-деревья обладают несколькими важными свойствами:

Они сбалансированы, то есть любую листовую страницу отделяет от корня одно и то же число внутренних страниц. Поэтому поиск любого значения занимает одинаковое время.
Они сильно ветвисты, то есть каждая страница (как правило, 8 КБ) содержит сразу много (сотни) TID-ов. За счет этого глубина B-деревьев получается небольшой; на практике до 4–5 для очень больших таблиц.
Данные в индексе упорядочены по неубыванию (как между страницами, так и внутри каждой страницы), а страницы одного уровня связаны между собой двунаправленным списком. Поэтому получить упорядоченный набор данных мы можем, просто проходя по списку в одну или в другую сторону, не возвращаясь каждый раз к корню.

2. **Поиск**: бинарный поиск по уровням дерева — логарифмическая сложность для B-tree.  
3. **Связь с heap**: индекс указывает на место строки в heap; после получения CTID сервер читает страницу heap и доставляет нужный tuple.  
4. **MVCC и видимость**: PostgreSQL — MVCC. Индекс не хранит информацию о видимости транзакций, поэтому после нахождения индекса сервер всё равно проверяет, видим ли tuple для текущей транзакции. Это важно для «index-only scan» (см. ниже).

# Index-only scan и Visibility Map (тонкость)
Postgres может выполнить **index-only scan**, когда все необходимые столбцы присутствуют в индексе: тогда нет необходимости читать heap-страницу — достаточно данных в индексе. Но это возможно **только если** видимость tuple известна из **visibility map** (специальная карта, которую поднимает VACUUM), иначе сервер всё равно читает heap, чтобы проверить видимость. Поэтому чтобы индекс-only действительно работал, нужно поддерживать актуальную visibility map (VACUUM, чтобы пометить страницы как «видимые»).

# Виды индексов по назначению (примеры и когда применять)
- **Уникальные индексы / PRIMARY KEY** — контроль уникальности. Создаются автоматически при ограничениях. (B-tree).  
- **Мультиколоночные индексы** — индекс по нескольким колонкам. Важное правило: используется **leftmost-prefix** — индекс на (a,b,c) может применяться как для поиска по (a), (a,b) или (a,b,c), но не для (b) без (a).  
- **INCLUDE (covering) columns** — начиная с PostgreSQL можно добавлять колонки в индекс как неключевые (INCLUDE), чтобы создать покрывающий индекс и сделать index-only scan, не добавляя их в ключ (не влияют на порядок). Пример: `CREATE INDEX idx ON t (a) INCLUDE (b, c);`.  
- **Partial index** — индекс не на всю таблицу, а на подмножество строк по условию. Отлично для редких статусов/флагов. Пример:

```sql
CREATE INDEX idx_active_users ON users (last_name) WHERE active IS TRUE;
```

Индекс используется только для запросов, содержащих такое же условие `WHERE`. Полезно экономить место и ускорять специфичные запросы.  
- **Expression index** — индекс не на сам столбец, а на выражение (например, `lower(email)`) — полезно при поиске нечувствительном к регистру:

```sql
CREATE INDEX idx_email_lower ON users (lower(email));
```

# Примеры SQL (практика)
```sql
-- обычный B-tree
CREATE INDEX idx_users_lastname ON users (last_name);

-- мультиколоночный (leftmost prefix)
CREATE INDEX idx_orders_customer_date ON orders (customer_id, order_date);

-- include / covering (Postgres 11+)
CREATE INDEX idx_orders_customer_date_include ON orders (customer_id, order_date) INCLUDE (total_amount);

-- partial index
CREATE INDEX idx_active_users ON users (last_name) WHERE active = true;

-- expression index (lowercase email)
CREATE INDEX idx_email_lower ON users (lower(email));

-- GIN для jsonb
CREATE INDEX idx_docs_data_gin ON documents USING gin (data jsonb_path_ops);

-- BRIN для очень большой таблицы, упорядоченной по времени
CREATE INDEX idx_events_time_brin ON events USING brin(event_time);
```

# Когда индекс используется планировщиком (подробно)
Планировщик оценивает **стоимость** — оценка количества строк, стоимости чтения страниц и т.д. Он выбирает между **index scan** и **sequential scan**. Если ожидается вернуть небольшой процент строк — индекс выгоден. Для больших выборок (порядок > ~10–30% — зависит от ситуации) последовательный скан часто быстрее. Также на решение влияют: статистика (ANALYZE), размер таблицы, фактор кэширования, селективность, наличие сортировки (ORDER BY может заставить использовать индекс для упрощения сортировки). Всегда проверяйте `EXPLAIN (ANALYZE, BUFFERS)` для реальных планов.

# Плюсы и минусы индексов
Плюсы:
- Значительно ускоряют чтение (первичный эффект).  
- Позволяют реализовать уникальные ограничения.  
- Могут обеспечить порядок (ORDER BY) без сортировки.  
- Специальные индексы (GIN/BRIN) сильно ускоряют специфичные типы запросов.  

Минусы:
- **Дополнительное пространство**: индексы занимают диск/память.  
- **Замедление записей**: каждый INSERT/UPDATE/DELETE должен поддерживать индексы — дополнительные I/O/CPU.  
- **Блоут (bloat)**: при частых обновлениях/удалениях индексы могут «раздуться», снижая эффективность; бывает нужно `REINDEX`/`VACUUM`/перестроение.  
- **Неправильные индексы**: слишком много индексов на таблице — обратный эффект для DML и обслуживания.  
- **Индекс не всегда помогает**: для низкой селективности (много одинаковых значений) или когда нужно почти весь набор строк — последовательный скан лучше.

# Тонкости и «подводные камни» (важно знать на собеседовании)
1. **Leftmost prefix rule (мультиколоночные индексы)** — см. выше; порядок колонок имеет значение.  
2. **Index-only scan зависит от visibility map** — если нет свежей информации в visibility map, Postgres будет читать heap, и index-only не сработает. Делайте `VACUUM`/`AUTOVACUUM`.  
3. **HOT updates (Heap-Only Tuple)**: если `UPDATE` не изменяет индексируемые колонки и есть свободное место на странице, Postgres может сделать HOT-update — это обновляет heap без изменения индекса, уменьшает нагрузку на индексы. Поэтому схемы, где часто обновляют неиндексируемые поля, выигрывают от HOT. Но если индексы меняются — операции дороже.  
4. **BRIN работает только при некоторой корреляции порядка** (например, данные вставляются по возрастанию по времени). Иначе BRIN будет малоэффективен.  
5. **GIN — медленный в обновлениях**: если у вас часто обновляются поля jsonb/array, GIN-индексы могут давать серьёзную нагрузку на запись. Иногда стоит использовать `fastupdate` настройку или отслеживать нагрузку.  
6. **Индексы и NULL**: B-tree не индексирует `NULL` в некоторых сценариях — но в Postgres `NULL` есть — нужно учитывать при условиях.  
7. **Статистика и ANALYZE**: планировщик опирается на статистику; если она старая/неполная — он может выбрать плохой план. Регулярно запускать `ANALYZE` (или довериться autovacuum/autostat).

# Как измерять и отлаживать (что показать на собеседовании)
- Используйте `EXPLAIN (ANALYZE, BUFFERS)` чтобы увидеть реальные затраты и количество чтений.  
- Сравните планы с/без индекса — удалите индекс и запустите `EXPLAIN` для понимания влияния.  
- Смотрите `pg_stat_user_indexes`, `pg_stat_all_indexes` для статистики использования индекса.  
- Для оценки размера: `pg_relation_size('index_name')` и `pg_total_relation_size`.  
- Для поиска блоута: `pgstattuple` extension и `REINDEX` при необходимости.

# Рекомендации (best practices)
1. Индексируйте **те колонки**, которые часто используются в `WHERE`, `JOIN`, `ORDER BY`, `GROUP BY`.  
2. Не индексируйте «малоселективные» булевы/категориальные колонки с очень маленьким числом значений — индекс там малополезен. Вместо этого — partial индекс для редких значений.  
3. Для частых запросов, занимающих только несколько колонок — делайте `INCLUDE`, чтобы получить index-only scans.  
4. Для JSONB/массивов — GIN. Но учитывайте накладные расходы на DML.  
5. Для очень больших лог-таблиц, где запросы по диапазону времени — BRIN может сэкономить место и ускорить запросы.  
6. Держите `ANALYZE`/`VACUUM` в рабочем состоянии (autovacuum), чтобы оптимизатор имел актуальную статистику и visibility map.  
7. Не создавайте множество частичных индексов, покрывающих разные значения одного столбца — часто лучше один многоколоночный индекс или рефакторинг.

# Примеры «интервью-вопросов» и как ответить кратко
1. *«Когда нужен индекс на внешнем ключе (FK)?»* — Обычно полезен: ускоряет JOIN и удаление/обновление родителя (если у ребёнка нет индекса, удаление родителя может делать seq scan и долго ждать).  
2. *«Почему index-only scan не сработал?»* — Потому что visibility map не показывает страницу как «all-visible» (нужен VACUUM) или потому что в запросе используются столбцы, не покрытые индексом.  
3. *«Почему Postgres выбрал sequential scan?»* — Планировщик посчитал, что seq scan дешевле (возможно большой процент строк; устаревшая статистика; маленькая таблица; кэш). Приведите `EXPLAIN (ANALYZE, BUFFERS)` как проверку.

# Операции обслуживания индексов (что бывает нужно)
- `VACUUM` и `ANALYZE` — поддерживают visibility map и статистику.  
- `REINDEX` — перестроит индекс целиком (удаляет блоат). Использовать при сильном раздутии/ошибках.  
- `CLUSTER` — физически перестроит таблицу по индексному порядку (дорого, но может улучшить последовательный доступ).  
- Мониторинг: `pg_stat_all_indexes`, `pg_stat_user_tables`, `pg_relation_size`.

# Типичные ошибки на собеседовании — что не упомянуть или переупростить
- Нельзя сказать «индекс всегда ускоряет» — важно упомянуть накладные расходы и сценарии, где индекс вреден.  
- Не игнорируйте MVCC — забудете про index-only scan и HOT обновления.  
- Не забудьте про leftmost prefix в мультиколоночных индексах.

# Краткий чек-лист при принятии решения «создавать ли индекс»
- Часто ли колонка участвует в `WHERE`/`JOIN`/`ORDER BY`? — если да, индекс полезен.  
- Какова селективность? (уникальность/распределение значений).  
- Как часто обновляются/вставляются строки? (частые DML снижают пользу индекса).  
- Можно ли покрыть запрос `INCLUDE`/expression index?  
- Таблица большая? Если да — подумайте о BRIN/GIN/частичных индексах.

# Полезные команды для диагностики (коротко)
```sql
EXPLAIN (ANALYZE, BUFFERS) SELECT ...;
-- размеры
SELECT pg_size_pretty(pg_relation_size('idx_name')), pg_size_pretty(pg_total_relation_size('table_name'));
-- статистика использования
SELECT * FROM pg_stat_user_indexes WHERE indexrelname='idx_name';
-- перестроить индекс
REINDEX INDEX idx_name;
-- посмотреть использованные планы и советы
```

# Заключение (что важно помнить на интервью)
- Поймите и объясните **плюсы/минусы** индексов; покажите знания MVCC, visibility map, index-only, HOT updates — это «очень хорошие» темы для собеседования.  
- Приводите примеры SQL и объясняйте почему индекс выбран или не выбран (покажите EXPLAIN).  
- Упомяните конкретные типы индексов и сценарии их применения (B-tree, GIN, GiST, BRIN и т.д.).

---

Отлично — ниже идёт серия **реальных, нетривиальных кейсов** (каждый — схема таблицы, типичные запросы, конкретные предложения по индексам с SQL, почему именно так, когда индекс не нужен и какие «подводные камни» учесть). Каждый пример рассчитан на интервью-уровень: хочу, чтобы вы могли не только назвать индекс, но и аргументировать выбор со ссылками на поведение PostgreSQL (MVCC, visibility map, HOT, BRIN/GIN особенности и т. д.).

# Формат каждого кейса
1. Схема (упрощённо)  
2. Типичные запросы / паттерны доступа  
3. Рекомендованный(е) индекс(ы) — SQL + зачем  
4. Когда индекс не нужен / плохо влияет  
5. Тонкости / как тестировать (EXPLAIN) и операции обслуживания

---

## Кейc 1 — Append-only лог событий для аналитики (very large time-series)
**Схема:**
```sql
CREATE TABLE event_log (
  id bigserial PRIMARY KEY,
  device_id bigint,
  event_time timestamptz NOT NULL,
  event_type text,
  payload jsonb
);
-- миллиарды строк, данные вставляются в основном по event_time в порядке возрастания
```

**Типичные запросы:**
- `SELECT * FROM event_log WHERE event_time BETWEEN X AND Y;`
- `SELECT * FROM event_log WHERE device_id = 123 AND event_time >= now() - interval '1 day';`
- агрегации по дням/часам

**Рекомендация индекса:**
- Если таблица **очень** большая (TB+), и вставки — в основном append по времени, **BRIN** гораздо выгоднее по месту и затратам, чем B-tree:
```sql
CREATE INDEX idx_event_time_brin ON event_log USING brin (event_time);
```
- Если часто фильтруют по `device_id` + `event_time` (малый диапазон для одного девайса), можно добавить **мультиколоночный B-tree** или комбинировать с BRIN:
```sql
CREATE INDEX idx_device_time ON event_log (device_id, event_time);
```
но это будет большой и дорогой в поддержке индекс при больших объемах вставок.

**Почему BRIN:** BRIN хранит агрегированную информацию по блокам страниц и дешев в размере; хорошо работает, когда есть корреляция значения столбца и физического расположения (вставки по времени). Если данные распределены случайно (нет корреляции), BRIN почти бесполезен.

**Когда индекс не нужен или вреден:**
- Если таблица не очень большая — B-tree быстрее для диапазонных выборок.  
- Если вы делаете интенсивные INSERTы и одновременно создаёте крупный B-tree по `event_time` — вставки станут медленнее и индекс будет занимать много места.  
- Если запросы почти всегда читают большой процент строк (например, ретроспективный экспорт) — seq scan может быть лучше.

**Тонкости / тестирование:**
- Проверяйте корреляцию колонки и физического порядка (`pg_stat_*`/анализ и EXPLAIN).  
- BRIN хорошо комбинируется с партиционированием по времени.

---

## Кейc 2 — Документы/метаданные с JSONB и сложными фильтрами (product catalog, search/filters)
**Схема:**
```sql
CREATE TABLE product (
  id bigserial PRIMARY KEY,
  sku text,
  title text,
  attributes jsonb, -- {"color":["red","blue"], "size":"M", "tags":["sale","new"]}
  price numeric,
  in_stock boolean
);
```

**Типичные запросы:**
- `SELECT id FROM product WHERE attributes @> '{"color":["red"]}' AND price BETWEEN 10 AND 50;`
- полнотекст/поиск по title + фильтрация по attributes
- частые INSERT/UPDATE (каталог обновляется регулярно)

**Рекомендация индексов:**
- `GIN` для `jsonb` для быстрого поиска по ключам/массивам:
```sql
CREATE INDEX idx_product_attr_gin ON product USING gin (attributes);
```
- Для полнотекстового поиска по `title` — `GIN` на `to_tsvector(title)`:
```sql
CREATE INDEX idx_product_title_tsv ON product USING gin (to_tsvector('russian', title));
```
- Если типичный запрос: `attributes @> '{"tags":["sale"]}' AND in_stock = true`, можно сделать **partial GIN** (или partial B-tree для `in_stock`), чтобы индекс покрывал лишь товары в наличии:
```sql
CREATE INDEX idx_product_attr_gin_instock ON product USING gin (attributes) WHERE in_stock = true;
```

**Почему так:** GIN — оптимален для структур с множественными значениями/ключами (jsonb/array). Partial index уменьшает размер индекса и делает поиск по узкой выборке более быстрым. Но GIN дорого обновлять — высокие накладные расходы при частых изменениях в `attributes`.

**Когда индекс не нужен / вреден:**
- Если `attributes` часто меняются для большинства товаров — GIN приведёт к высокой нагрузке на INSERT/UPDATE. В этом случае подумайте о:
  - денормализации часто-используемых атрибутов в отдельные столбцы (и B-tree).
  - отложенной индексации / background reindex (но это сложно).
- Для выбора по `price BETWEEN` используйте B-tree по `price` только если такие запросы селективны; иначе seq scan + агрегирование может быть дешевле.

**Тонкости / тестирование:**
- Включите `EXPLAIN (ANALYZE)` и оцените влияние GIN на скорость записи. Рассмотрите `gin_fastupdate` настройку (по умолчанию true) и мониторинг.

---

## Кейc 3 — E-commerce orders: частые чтения по пользователю + сортировка по дате
**Схема:**
```sql
CREATE TABLE orders (
  id bigserial PRIMARY KEY,
  user_id bigint NOT NULL,
  status text, -- 'new','shipped','cancelled'...
  created_at timestamptz NOT NULL,
  total_amount numeric
);
```

**Типичные запросы:**
- `SELECT * FROM orders WHERE user_id = :uid ORDER BY created_at DESC LIMIT 20;` (на страницу заказов)
- `SELECT count(*) FROM orders WHERE status = 'new';` (оперативная метрика)
- JOIN с users по user_id

**Рекомендация индексов:**
- Для быстрых «последних заказов пользователя» — мультиколоночный B-tree `(user_id, created_at DESC)` (order-aware):
```sql
CREATE INDEX idx_orders_user_date ON orders (user_id, created_at DESC);
```
- Для быстрых подсчётов по статусу: **partial index** полезен, если `status='new'` — редко:
```sql
CREATE INDEX idx_orders_status_new ON orders (user_id) WHERE status = 'new';
-- или на сам status для агрегаций если селективность высокая:
CREATE INDEX idx_orders_status ON orders (status);
```
- Если часто возвращают `total_amount` вместе с `user_id, created_at`, можно добавить `INCLUDE (total_amount)` чтобы получить **index-only scan**:
```sql
CREATE INDEX idx_orders_user_date_inc ON orders (user_id, created_at DESC) INCLUDE (total_amount);
```

**Почему так:** мултиколоночный индекс покрывает поисковый префикс + упорядочивание; `INCLUDE` позволяет покрыть запрос и избежать чтения heap (index-only), если visibility map показывает страницы «all-visible».

**Когда индекс не нужен / вреден:**
- Если у пользователя типично десятки миллионов заказов (маловероятно), индекс всё равно полезен. Но если почти все запросы выполняют скан по status (например, отчёты по всем заказам), то отдельный индекс по `status` может быть лишним — лучше агрегации со сканированием/партиционированием.
- Не делайте отдельный индекс на `user_id` и ещё один на `(user_id, created_at)` — второй покрывает первый из-за leftmost-prefix.

**Тонкости / тестирование:**
- Учитывайте `DESC` в индексе (создание индекса с нужным направлением убирает отдельную сортировку).  
- Проверяйте `EXPLAIN (ANALYZE)` чтобы убедиться, что plan использует индекс для LIMIT (index scan with backward scan).

---

## Кейc 4 — Таблица с флагами и редкими состояниями (feature toggles, support tickets)
**Схема:**
```sql
CREATE TABLE support_ticket (
  id bigserial,
  user_id bigint,
  status text, -- 'open','in_progress','resolved','archived'
  priority smallint, -- 0..10
  created_at timestamptz
);
-- Пример: 95% — status='resolved', 5% — 'open'/'in_progress'
```

**Типичные запросы:**
- `SELECT * FROM support_ticket WHERE status = 'open' ORDER BY created_at;`
- `SELECT count(*) FROM support_ticket WHERE priority >= 8 AND status != 'resolved';`

**Рекомендация индексов:**
- Для редких статусов (`open` / `in_progress`) — **partial index** выгоднее, чем полнотабличный индекс:
```sql
CREATE INDEX idx_ticket_open_created ON support_ticket (created_at) WHERE status IN ('open', 'in_progress');
```
- Не индексируйте `status` полностью, если он имеет очень низкую селективность (многие повторяющиеся значения).

**Почему так:** partial индекс покрывает редкий набор и компактнее/быстрее для соответствующих запросов. Полный индекс по `status` будет иметь низкую селективность и почти не поможет, зато замедлит DML.

**Когда индекс не нужен / вреден:**
- Не стоит индексировать булевые/низкоселективные колонки целиком (например `is_active` если 99% true). Вместо этого — partial index на редкую ветку (WHERE is_active = false).  
- Избыточные индексы на те же колонки (разные вариации) ухудшают INSERT/UPDATE.

**Тонкости / тестирование:**
- Подумайте о порядке колонок в индексе и возможности использования leftmost prefix.  
- Проверяйте частоту DML и планировщик (ANALYZE).

---

## Кейc 5 — Многомиллионная таблица с отношениями many-to-many (likes, follows)
**Схема:**
```sql
CREATE TABLE user_follow (
  follower_id bigint,
  followee_id bigint,
  created_at timestamptz,
  PRIMARY KEY (follower_id, followee_id) -- может быть либо PK либо уникальный индекс
);
-- Ожидается миллионы записей; операции: add/remove follow, list followers/followees
```

**Типичные запросы:**
- `SELECT followee_id FROM user_follow WHERE follower_id = :uid LIMIT 100;` (список тех, кого юзер читает)
- `SELECT follower_id FROM user_follow WHERE followee_id = :uid LIMIT 100;` (список подписчиков)
- `DELETE FROM user_follow WHERE follower_id = :a AND followee_id = :b;`

**Рекомендация индексов:**
- Если PK `(follower_id, followee_id)` — он удобен для быстрых проверок и удаления конкретной пары.  
- Но для быстрого получения **followers** по `followee_id` нужен **второй индекс**:
```sql
CREATE INDEX idx_user_follow_followee ON user_follow (followee_id, follower_id);
```
(мультиколоночный с правой колонкой follower_id может помочь при ORDER BY по created_at, если добавить её).

**Почему так:** PK даёт быстрый lookup для проверки существования и удаления, но не покрывает обратный запрос (followers) — нужен отдельный индекс. При этом два индекса — дополнительные расходы на вставку/удаление.  

**Когда индекс не нужен / вреден:**
- Если приложение редко использует обратный lookup (followers), можно его не добавлять и выполнять реже дорогостоящие запросы.  
- Но отсутствие индекса делает `SELECT ... WHERE followee_id = :uid` дорогостоящим (seq scan), что влияет на latency.

**Тонкости / оптимизация:**
- Используйте `UNLOGGED`/IN-MEM tables для временных графов, если допустима потеря после restart.  
- Рассмотрите партицирование или хранение follower списка в denormalized structures (Redis) при экстремальной нагрузке.

---

## Кейc 6 — Каталог с нечувствительным к регистру поиском по e-mail (функциональный индекс)
**Схема:**
```sql
CREATE TABLE users (
  id bigserial,
  email text UNIQUE,
  name text
);
-- Частые WHERE LOWER(email) = '...'
```

**Типичные запросы:**
- `SELECT * FROM users WHERE lower(email) = lower(:email);`

**Рекомендация индексов:**
- **Expression index** на `lower(email)`:
```sql
CREATE UNIQUE INDEX idx_users_email_lower ON users (lower(email));
```
(если хотите сохранить уникальность без чувствительности к регистру)

**Почему так:** индекс на выражении позволяет использовать индекс при поиске по `lower(email)`; простого индекса на `email` не хватит для `lower()`-поиска.

**Когда индекс не нужен / вреден:**
- Если все запросы используют точный поиск по `email` (без lower), достаточно обычного уникального B-tree индекса.  
- Не создавайте оба индекса без причины.

**Тонкости / тестирование:**
- Убедитесь, что запрос использует ту же форму выражения, что и индекс (точное совпадение выражения).

---

## Кейc 7 — Таблица audit_log: частые UPDATE метаданных, но индексы по редко меняющимся полям
**Схема:**
```sql
CREATE TABLE audit_log (
  id bigserial,
  entity_id bigint,
  change_type text,
  meta jsonb, -- часто обновляется
  created_at timestamptz
);
-- Поле meta меняется часто; индекс нужен по entity_id+created_at фильтрованным запросам
```

**Типичные запросы:**
- `SELECT * FROM audit_log WHERE entity_id = :id ORDER BY created_at DESC LIMIT 50;`

**Рекомендация индексов:**
- B-tree `(entity_id, created_at DESC)` с `INCLUDE` если нужно возвращать небольшие колонки:
```sql
CREATE INDEX idx_audit_entity_date_inc ON audit_log (entity_id, created_at DESC) INCLUDE (change_type);
```

**Почему так:** если `meta` часто обновляется, но индексируемые колонки не меняются — **HOT updates** возможны и помогут избежать дополнительных изменений индекса (если обновление не трогает индексируемые столбцы) — это ускорит DML. Но если обновлять индексируемые колонки, то индексы придётся обновлять.

**Когда индекс не нужен / вреден:**
- Если `meta` — единственная важная колонка и она часто меняется, индексация по ней бессмысленна.  
- Если `entity_id` очень низко-селективен (многие записи на одну entity), подумайте о партиционировании.

**Тонкости / обслуживание:**
- Настройте `fillfactor` для таблицы, если много версий строк и хотите увеличить шанс HOT (оставить немного места на странице).  
- Мониторьте `pg_stat_all_indexes` и bloat; при необходимости `REINDEX`.

---

# Общие «интервьюные» доводы и проверки (как аргументировать выбор индекса)
1. **Селективность > 5–10%?** — индекс может помочь; иначе seq scan чаще выгоднее. (точный порог зависит от таблицы и кэша).  
2. **Leftmost-prefix**: объясните, почему `(a,b,c)` индекс не помогает запросу по `(b)` без `(a)`. Приведите пример.  
3. **Index-only scan**: если все нужные колонки находятся в индексе и visibility map помечает страницы как all-visible — снимаем чтение heap. Для этого нужен `VACUUM` и правильно настроенный autovacuum.  
4. **HOT обновления**: если часто обновляются неиндексируемые колонки — PostgreSQL сможет делать HOT и не трогать индексы (ускорит DML). Если же обновляются индексируемые колонки — индексы придётся обновлять.  
5. **BRIN** — польза при корреляции колонка↔физический порядок (append-only). Если корреляции нет — BRIN бесполезен.  
6. **GIN** — супер для jsonb/массивов/tsvector, но дорог по времени INSERT/UPDATE.

# Советы по тестированию в интервью / на практике
- Всегда демонстрируйте `EXPLAIN (ANALYZE, BUFFERS)` до/после создания индекса.  
- Демонстрируйте влияние на вставки: замеряйте `INSERT`/`UPDATE` throughput с/без индекса (simple benchmark).  
- Покажите проверку использования индекса: `pg_stat_user_indexes` для мониторинга `idx_scan`, `pg_relation_size` для размера.

# Краткое резюме — полезная шпаргалка
- BRIN — большие append-only таблицы с корреляцией (time series).  
- GIN — jsonb, массивы, tsvector. Быстро читать — медленно писать.  
- Partial — экономит место для редких значений (флаги/статусы).  
- INCLUDE / index-only — уменьшает чтение heap (нужен visibility map + VACUUM).  
- HOT updates — уменьшает работу по индексам при обновлении неиндексируемых полей.

---


[Вопросы для собеседования](README.md)

# SQL
+ [Что такое _«SQL»_?](#Что-такое-sql)
+ [Какие существуют операторы SQL?](#Какие-существуют-операторы-sql)
+ [Что означает `NULL` в SQL?](#Что-означает-null-в-sql)
+ [Что такое _«временная таблица»_? Для чего она используется?](#Что-такое-временная-таблица-Для-чего-она-используется)
+ [Что такое _«представление» (view)_ и для чего оно применяется?](#Что-такое-представление-view-и-для-чего-оно-применяется)
+ [Каков общий синтаксис оператора `SELECT`?](#Каков-общий-синтаксис-оператора-select)
+ [Что такое `JOIN`?](#Что-такое-join)
+ [Какие существуют типы `JOIN`?](#Какие-существуют-типы-join)
+ [Что лучше использовать `JOIN` или подзапросы?](#Что-лучше-использовать-join-или-подзапросы)
+ [Для чего используется оператор `HAVING`?](#Для-чего-используется-оператор-having)
+ [В чем различие между операторами `HAVING` и `WHERE`?](#В-чем-различие-между-операторами-having-и-where)
+ [Для чего используется оператор `ORDER BY`?](#Для-чего-используется-оператор-order-by)
+ [Для чего используется оператор `GROUP BY`?](#Для-чего-используется-оператор-group-by)
+ [Как `GROUP BY` обрабатывает значение `NULL`?](#Как-group-by-обрабатывает-значение-null)
+ [В чем разница между операторами `GROUP BY` и `DISTINCT`?](#В-чем-разница-между-операторами-group-by-и-distinct)
+ [Перечислите основные агрегатные функции.](#Перечислите-основные-агрегатные-функции)
+ [В чем разница между `COUNT(*)` и `COUNT({column})`?](#В-чем-разница-между-count-и-countcolumn)
+ [Что делает оператор `EXISTS`?](#Что-делает-оператор-exists)
+ [Для чего используются операторы `IN`, `BETWEEN`, `LIKE`?](#Для-чего-используются-операторы-in-between-like)
+ [Для чего применяется ключевое слово `UNION`?](#Для-чего-применяется-ключевое-слово-union)
+ [Какие ограничения на целостность данных существуют в SQL?](#Какие-ограничения-на-целостность-данных-существуют-в-sql)
+ [Какие отличия между ограничениями `PRIMARY` и `UNIQUE`?](#Какие-отличия-между-ограничениями-primary-и-unique)
+ [Может ли значение в столбце, на который наложено ограничение `FOREIGN KEY`, равняться `NULL`?](#Может-ли-значение-в-столбце-на-который-наложено-ограничение-foreign-key-равняться-null)
+ [Как создать индекс?](#Как-создать-индекс)
+ [Что делает оператор `MERGE`?](#Что-делает-оператор-merge)
+ [В чем отличие между операторами `DELETE` и `TRUNCATE`?](#В-чем-отличие-между-операторами-delete-и-truncate)
+ [Что такое _«хранимая процедура»_?](#Что-такое-хранимая-процедура)
+ [Что такое _«триггер»_?](#Что-такое-триггер)
+ [Что такое _«курсор»_?](#Что-такое-курсор)
+ [Опишите разницу типов данных `DATETIME` и `TIMESTAMP`.](#Опишите-разницу-типов-данных-datetime-и-timestamp)
+ [Для каких числовых типов недопустимо использовать операции сложения/вычитания?](#Для-каких-числовых-типов-недопустимо-использовать-операции-сложениявычитания)
+ [Какое назначение у операторов `PIVOT` и `UNPIVOT` в Transact-SQL?](#Какое-назначение-у-операторов-pivot-и-unpivot-в-transact-sql)
+ [Расскажите об основных функциях ранжирования в Transact-SQL.](#Расскажите-об-основных-функциях-ранжирования-в-transact-sql)
+ [Для чего используются операторы `INTERSECT`, `EXCEPT` в Transact-SQL?](#Для-чего-используются-операторы-intersect-except-в-transact-sql)
+ [Напишите запрос...](#Напишите-запрос)

## Что такое _«SQL»_?
SQL, Structured query language («язык структурированных запросов») — формальный непроцедурный язык программирования, применяемый для создания, модификации и управления данными в произвольной реляционной базе данных, управляемой соответствующей системой управления базами данных (СУБД).

[к оглавлению](#sql)

## Какие существуют операторы SQL?
__операторы определения данных (Data Definition Language, DDL)__:

+ `CREATE` создает объект БД (базу, таблицу, представление, пользователя и т. д.),
+ `ALTER` изменяет объект,
+ `DROP` удаляет объект;

__операторы манипуляции данными (Data Manipulation Language, DML)__:

+ `SELECT` выбирает данные, удовлетворяющие заданным условиям,
+ `INSERT` добавляет новые данные,
+ `UPDATE` изменяет существующие данные,
+ `DELETE` удаляет данные;

__операторы определения доступа к данным (Data Control Language, DCL)__:

+ `GRANT` предоставляет пользователю (группе) разрешения на определенные операции с объектом,
+ `REVOKE` отзывает ранее выданные разрешения,
+ `DENY` задает запрет, имеющий приоритет над разрешением;

__операторы управления транзакциями (Transaction Control Language, TCL)__:

+ `COMMIT` применяет транзакцию,
+ `ROLLBACK` откатывает все изменения, сделанные в контексте текущей транзакции,
+ `SAVEPOINT` разбивает транзакцию на более мелкие.

[к оглавлению](#sql)

## Что означает `NULL` в SQL?
`NULL` - специальное значение (псевдозначение), которое может быть записано в поле таблицы базы данных. NULL соответствует понятию «пустое поле», то есть «поле, не содержащее никакого значения».

`NULL` означает отсутствие, неизвестность информации. Значение `NULL` не является значением в полном смысле слова: по определению оно означает отсутствие значения и не принадлежит ни одному типу данных. Поэтому `NULL` не равно ни логическому значению `FALSE`, ни _пустой строке_, ни `0`. При сравнении `NULL` с любым значением будет получен результат `NULL`, а не `FALSE` и не `0`. Более того, `NULL` не равно `NULL`!

[к оглавлению](#sql)

## Что такое _«временная таблица»_? Для чего она используется?
__Временная таблица__ - это объект базы данных, который хранится и управляется системой базы данных на временной основе. Они могут быть локальными или глобальными. Используется для сохранения результатов вызова хранимой процедуры, уменьшение числа строк при соединениях, агрегирование данных из различных источников или как замена курсоров и параметризованных представлений.

[к оглавлению](#sql)

## Что такое _«представление» (view)_ и для чего оно применяется?
__Представление__, View - виртуальная таблица, представляющая данные одной или более таблиц альтернативным образом.

В действительности представление – всего лишь результат выполнения оператора `SELECT`, который хранится в структуре памяти, напоминающей SQL таблицу. Они работают в запросах и операторах DML точно также как и основные таблицы, но не содержат никаких собственных данных. Представления значительно расширяют возможности управления данными. Это способ дать публичный доступ к некоторой (но не всей) информации в таблице.

[к оглавлению](#sql)

## Каков общий синтаксис оператора `SELECT`?
`SELECT` - оператор DML SQL, возвращающий набор данных (выборку) из базы данных, удовлетворяющих заданному условию. Имеет следующую структуру:

```sql
SELECT 
       [DISTINCT | DISTINCTROW | ALL]
       select_expression,...
   FROM table_references
     [WHERE where_definition]
     [GROUP BY {unsigned_integer | column | formula}]
     [HAVING where_definition]
     [ORDER BY {unsigned_integer | column | formula} [ASC | DESC], ...]
```

[к оглавлению](#sql)

## Что такое `JOIN`?
__JOIN__ - оператор языка SQL, который является реализацией операции соединения реляционной алгебры. Предназначен для обеспечения выборки данных из двух таблиц и включения этих данных в один результирующий набор. 

Особенностями операции соединения являются следующее:

+ в схему таблицы-результата входят столбцы обеих исходных таблиц (таблиц-операндов), то есть схема результата является «сцеплением» схем операндов;
+ каждая строка таблицы-результата является «сцеплением» строки из одной таблицы-операнда со строкой второй таблицы-операнда;
+ при необходимости соединения не двух, а нескольких таблиц, операция соединения применяется несколько раз (последовательно).

```sql
SELECT
  field_name [,... n]
FROM
  Table1
  {INNER | {LEFT | RIGHT | FULL} OUTER | CROSS } JOIN
  Table2
    {ON <condition> | USING (field_name [,... n])}
```

[к оглавлению](#sql)

## Какие существуют типы `JOIN`?
__(INNER) JOIN__
Результатом объединения таблиц являются записи, общие для левой и правой таблиц. Порядок таблиц для оператора не важен, поскольку оператор является симметричным.

__LEFT (OUTER) JOIN__
Производит выбор всех записей первой таблицы и соответствующих им записей второй таблицы. Если записи во второй таблице не найдены, то вместо них подставляется пустой результат (`NULL`). Порядок таблиц для оператора важен, поскольку оператор не является симметричным.

__RIGHT (OUTER) JOIN__
`LEFT JOIN` с операндами, расставленными в обратном порядке. Порядок таблиц для оператора важен, поскольку оператор не является симметричным.

__FULL (OUTER) JOIN__
Результатом объединения таблиц являются все записи, которые присутствуют в таблицах. Порядок таблиц для оператора не важен, поскольку оператор является симметричным.

__CROSS JOIN (декартово произведение)__
При выборе каждая строка одной таблицы объединяется с каждой строкой второй таблицы, давая тем самым все возможные сочетания строк двух таблиц. Порядок таблиц для оператора не важен, поскольку оператор является симметричным.

[к оглавлению](#sql)

## Что лучше использовать `JOIN` или подзапросы?
Обычно лучше использовать `JOIN`, поскольку в большинстве случаев он более понятен и лучше оптимизируется СУБД (но 100% этого гарантировать нельзя). Так же `JOIN` имеет заметное преимущество над подзапросами в случае, когда список выбора `SELECT` содержит столбцы более чем из одной таблицы.

Подзапросы лучше использовать в случаях, когда нужно вычислять агрегатные значения и использовать их для сравнений во внешних запросах.

[к оглавлению](#sql)

## Для чего используется оператор `HAVING`?
`HAVING` используется для фильтрации результата `GROUP BY` по заданным логическим условиям.

[к оглавлению](#sql)

## В чем различие между операторами `HAVING` и `WHERE`?
Основное отличие 'WHERE' от 'HAVING' заключается в том, что 'WHERE' сначала выбирает строки, а затем группирует их и вычисляет агрегатные функции (таким образом, она отбирает строки для вычисления агрегатов), тогда как 'HAVING' отбирает строки групп после группировки и вычисления агрегатных функций. Как следствие, предложение 'WHERE' не должно содержать агрегатных функций; не имеет смысла использовать агрегатные функции для определения строк для вычисления агрегатных функций. Предложение 'HAVING', напротив, всегда содержит агрегатные функции. (Строго говоря, вы можете написать предложение 'HAVING', не используя агрегаты, но это редко бывает полезно. То же самое условие может работать более эффективно на стадии 'WHERE'.)

[к оглавлению](#sql)

## Для чего используется оператор `ORDER BY`?
__ORDER BY__ упорядочивает вывод запроса согласно значениям в том или ином количестве выбранных столбцов. Многочисленные столбцы упорядочиваются один внутри другого. Возможно определять возрастание `ASC` или убывание `DESC` для каждого столбца. По умолчанию установлено - возрастание.

[к оглавлению](#sql)

## Для чего используется оператор `GROUP BY`?
`GROUP BY` используется для агрегации записей результата по заданным признакам-атрибутам.

[к оглавлению](#sql)

## Как `GROUP BY` обрабатывает значение `NULL`?
При использовании `GROUP BY` все значения `NULL` считаются равными.

[к оглавлению](#sql)

## В чем разница между операторами `GROUP BY` и `DISTINCT`?
`DISTINCT` указывает, что для вычислений используются только уникальные значения столбца. `NULL` считается как отдельное значение. 
`GROUP BY` создает отдельную группу для всех возможных значений (включая значение `NULL`). 

Если нужно удалить только дубликаты лучше использовать `DISTINCT`, `GROUP BY` лучше использовать для определения групп записей, к которым могут применяться агрегатные функции.

[к оглавлению](#sql)

## Перечислите основные агрегатные функции.
__Агрегатных функции__ - функции, которые берут группы значений и сводят их к одиночному значению. 

SQL предоставляет несколько агрегатных функций:

`COUNT` - производит подсчет записей, удовлетворяющих условию запроса;
`SUM` - вычисляет арифметическую сумму всех значений колонки;
`AVG` - вычисляет среднее арифметическое всех значений;
`MAX` - определяет наибольшее из всех выбранных значений;
`MIN` - определяет наименьшее из всех выбранных значений.

[к оглавлению](#sql)

## В чем разница между `COUNT(*)` и `COUNT({column})`?
`COUNT (*)` подсчитывает количество записей в таблице, не игнорируя значение NULL, поскольку эта функция оперирует записями, а не столбцами.

`COUNT ({column})` подсчитывает количество значений в `{column}`. При подсчете количества значений столбца эта форма функции `COUNT` не принимает во внимание значение `NULL`.

[к оглавлению](#sql)

## Что делает оператор `EXISTS`?
`EXISTS` берет подзапрос, как аргумент, и оценивает его как `TRUE`, если подзапрос возвращает какие-либо записи и `FALSE`, если нет.

[к оглавлению](#sql)

## Для чего используются операторы `IN`, `BETWEEN`, `LIKE`?
`IN` - определяет набор значений.

```sql 
SELECT * FROM Persons WHERE name IN ('Ivan','Petr','Pavel');
```

`BETWEEN` определяет диапазон значений. В отличие от `IN`, `BETWEEN` чувствителен к порядку, и первое значение в предложении должно быть первым по алфавитному или числовому порядку.

```sql 
SELECT * FROM Persons WHERE age BETWEEN 20 AND 25;
```

`LIKE` применим только к полям типа `CHAR` или `VARCHAR`, с которыми он используется чтобы находить подстроки. В качестве условия используются _символы шаблонизации (wildkards_) - специальные символы, которые могут соответствовать чему-нибудь: 

+ `_` замещает любой одиночный символ. Например, `'b_t'` будет соответствовать словам `'bat'` или `'bit'`, но не будет соответствовать `'brat'`. 

+ `%` замещает последовательность любого числа символов. Например `'%p%t'` будет соответствовать словам `'put'`, `'posit'`, или `'opt'`, но не `'spite'`.

```sql 
SELECT * FROM UNIVERSITY WHERE NAME LIKE '%o';
```

[к оглавлению](#sql)

## Для чего применяется ключевое слово `UNION`?
В языке SQL ключевое слово `UNION` применяется для объединения результатов двух SQL-запросов в единую таблицу, состоящую из схожих записей. Оба запроса должны возвращать одинаковое число столбцов и совместимые типы данных в соответствующих столбцах. Необходимо отметить, что `UNION` сам по себе не гарантирует порядок записей. Записи из второго запроса могут оказаться в начале, в конце или вообще перемешаться с записями из первого запроса. В случаях, когда требуется определенный порядок, необходимо использовать `ORDER BY`.

[к оглавлению](#sql)

## Какие ограничения на целостность данных существуют в SQL?
`PRIMARY KEY` - набор полей (1 или более), значения которых образуют уникальную комбинацию и используются для однозначной идентификации записи в таблице. Для таблицы может быть создано только одно такое ограничение. Данное ограничение используется для обеспечения целостности сущности, которая описана таблицей.

`CHECK` используется для ограничения множества значений, которые могут быть помещены в данный столбец. Это ограничение используется для обеспечения целостности предметной области, которую описывают таблицы в базе.

`UNIQUE` обеспечивает отсутствие дубликатов в столбце или наборе столбцов.

`FOREIGN KEY` защищает от действий, которые могут нарушить связи между таблицами. `FOREIGN KEY` в одной таблице указывает на `PRIMARY KEY` в другой. Поэтому данное ограничение нацелено на то, чтобы не было записей `FOREIGN KEY`, которым не отвечают записи `PRIMARY KEY`.

[к оглавлению](#sql)

## Какие отличия между ограничениями `PRIMARY` и `UNIQUE`?
По умолчанию ограничение `PRIMARY` создает кластерный индекс на столбце, а `UNIQUE` - некластерный. Другим отличием является то, что `PRIMARY` не разрешает `NULL` записей, в то время как `UNIQUE` разрешает одну (а в некоторых СУБД несколько) `NULL` запись.

[к оглавлению](#sql)

## Может ли значение в столбце, на который наложено ограничение `FOREIGN KEY`, равняться `NULL`?
Может, если на данный столбец не наложено ограничение `NOT NULL`. 

[к оглавлению](#sql)

## Как создать индекс? 
Индекс можно создать либо с помощью выражения `CREATE INDEX`: 
```sql
CREATE INDEX index_name ON table_name (column_name)
```

либо указав ограничение целостности в виде уникального `UNIQUE` или первичного `PRIMARY` ключа в операторе создания таблицы `CREATE TABLE`.

[к оглавлению](#sql)

## Что делает оператор `MERGE`?
`MERGE` позволяет осуществить слияние данных одной таблицы с данными другой таблицы. При слиянии таблиц проверяется условие, и если оно истинно, то выполняется `UPDATE`, а если нет - `INSERT`. При этом изменять поля таблицы в секции `UPDATE`, по которым идет связывание двух таблиц, нельзя.

[к оглавлению](#sql)

## В чем отличие между операторами `DELETE` и `TRUNCATE`?
`DELETE` - оператор DML, удаляет записи из таблицы, которые удовлетворяют критерию `WHERE` при этом задействуются триггеры, ограничения и т.д.

`TRUNCATE` - DDL оператор (удаляет таблицу и создает ее заново. Причем если на эту таблицу есть ссылки `FOREGIN KEY` или таблица используется в репликации, то пересоздать такую таблицу не получится).

[к оглавлению](#sql)

## Что такое _«хранимая процедура»_?
__Хранимая процедура__ — объект базы данных, представляющий собой набор SQL-инструкций, который хранится на сервере. Хранимые процедуры очень похожи на обыкновенные процедуры языков высокого уровня, у них могут быть входные и выходные параметры и локальные переменные, в них могут производиться числовые вычисления и операции над символьными данными, результаты которых могут присваиваться переменным и параметрам. В хранимых процедурах могут выполняться стандартные операции с базами данных (как DDL, так и DML). Кроме того, в хранимых процедурах возможны циклы и ветвления, то есть в них могут использоваться инструкции управления процессом исполнения.

Хранимые процедуры позволяют повысить производительность, расширяют возможности программирования и поддерживают функции безопасности данных. В большинстве СУБД при первом запуске хранимой процедуры она компилируется (выполняется синтаксический анализ и генерируется план доступа к данным) и в дальнейшем её обработка осуществляется быстрее.

[к оглавлению](#sql)

## Что такое _«триггер»_?
__Триггер (trigger)__ — это хранимая процедура особого типа, которую пользователь не вызывает непосредственно, а исполнение которой обусловлено действием по модификации данных: добавлением, удалением или изменением данных в заданной таблице реляционной базы данных. Триггеры применяются для обеспечения целостности данных и реализации сложной бизнес-логики. Триггер запускается сервером автоматически и все производимые им модификации данных рассматриваются как выполняемые в транзакции, в которой выполнено действие, вызвавшее срабатывание триггера. Соответственно, в случае обнаружения ошибки или нарушения целостности данных может произойти откат этой транзакции.

Момент запуска триггера определяется с помощью ключевых слов `BEFORE` (триггер запускается до выполнения связанного с ним события) или `AFTER` (после события). В случае, если триггер вызывается до события, он может внести изменения в модифицируемую событием запись. Кроме того, триггеры могут быть привязаны не к таблице, а к представлению (VIEW). В этом случае с их помощью реализуется механизм «обновляемого представления». В этом случае ключевые слова `BEFORE` и `AFTER` влияют лишь на последовательность вызова триггеров, так как собственно событие (удаление, вставка или обновление) не происходит.

[к оглавлению](#sql)

## Что такое _«курсор»_?
__Курсор__ — это объект базы данных, который позволяет приложениям работать с записями «по-одной», а не сразу с множеством, как это делается в обычных SQL командах.

Порядок работы с курсором такой:

+ Определить курсор (`DECLARE`)
+ Открыть курсор (`OPEN`)
+ Получить запись из курсора (`FETCH`)
+ Обработать запись...
+ Закрыть курсор (`CLOSE`)
+ Удалить ссылку курсора (`DEALLOCATE`). Когда удаляется последняя ссылка курсора, SQL освобождает структуры данных, составляющие курсор.

[к оглавлению](#sql)

## Опишите разницу типов данных `DATETIME` и `TIMESTAMP`.
`DATETIME` предназначен для хранения целого числа: `YYYYMMDDHHMMSS`. И это время не зависит от временной зоны, настроенной на сервере.
Размер: 8 байт

`TIMESTAMP` хранит значение равное количеству секунд, прошедших с полуночи 1 января 1970 года по усреднённому времени Гринвича. При получении из базы отображается с учётом часового пояса. Размер: 4 байта

[к оглавлению](#sql)

## Для каких числовых типов недопустимо использовать операции сложения/вычитания?
В качестве операндов операций сложения и вычитания нельзя использовать числовой тип `BIT`.

[к оглавлению](#sql)

## Какое назначение у операторов `PIVOT` и `UNPIVOT` в Transact-SQL?
`PIVOT` и `UNPIVOT` являются нестандартными реляционными операторами, которые поддерживаются Transact-SQL. 

Оператор `PIVOT` разворачивает возвращающее табличное значение выражение, преобразуя уникальные значения одного столбца выражения в несколько выходных столбцов, а также, в случае необходимости, объединяет оставшиеся повторяющиеся значения столбца и отображает их в выходных данных. Оператор `UNPIVOT` производит действия, обратные `PIVOT`, преобразуя столбцы возвращающего табличное значение выражения в значения столбца.

[к оглавлению](#sql)

## Расскажите об основных функциях ранжирования в Transact-SQL.
Ранжирующие функции - это функции, которые возвращают значение для каждой записи группы в результирующем наборе данных. На практике они могут быть использованы, например, для простой нумерации списка, составления рейтинга или постраничной навигации.

К примеру, у нас имеется набор данных следующего вида:

![ ](images/SQL/image.png)

`ROW_NUMBER` – функция нумерации в Transact-SQL, которая возвращает просто номер записи.

Например, запрос 
```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       ROW_NUMBER() OVER(ORDER BY Marks) RowNumber
FROM ExamResult;
```
Вернёт набор данных следующего вида:

![ ](images/SQL/row_number-sql-rank-function.png)

А запрос вида
```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       ROW_NUMBER() OVER(ORDER BY Marks desc) RowNumber
FROM ExamResult;
```

Вернёт набор

![ ](images/SQL/row_number-example.png)


`RANK` возвращает ранг каждой записи. В данном случае, в отличие от `ROW_NUMBER`, идет уже анализ значений и в случае нахождения одинаковых возвращает одинаковый ранг с пропуском следующего.

Например:

```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       RANK() OVER(PARTITION BY Studentname ORDER BY Marks DESC) Rank
FROM ExamResult
ORDER BY Studentname, 
         Rank;
```

Результат:

![ ](images/SQL/ranksql-rank-function.png)

Ещё пример:

```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       RANK() OVER(ORDER BY Marks DESC) Rank
FROM ExamResult
ORDER BY Rank;
```

Результат:

![ ](images/SQL/output-of-rank-function-for-similar-values.png)


`DENSE_RANK` так же возвращает ранг каждой записи, но в отличие от `RANK` в случае нахождения одинаковых значений возвращает ранг без пропуска следующего.

Например:

```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       DENSE_RANK() OVER(ORDER BY Marks DESC) Rank
FROM ExamResult
ORDER BY Rank;
```

Результат:

![ ](images/SQL/dense_ranksql-rank-function.png)

Ещё пример:

```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       DENSE_RANK() OVER(PARTITION BY Subject ORDER BY Marks DESC) Rank
FROM ExamResult
ORDER BY Studentname, 
         Rank;
```

Результат:

![ ](images/SQL/output-of-dense_rank-function.png)

Ну, и на последок, продемонстрируем разницу между `DENSE_RANK` и `RANK`:

```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       RANK() OVER(PARTITION BY StudentName ORDER BY Marks ) Rank
FROM ExamResult
ORDER BY Studentname, 
         Rank;
```


```sql
SELECT Studentname, 
       Subject, 
       Marks, 
       DENSE_RANK() OVER(PARTITION BY StudentName ORDER BY Marks ) Rank
FROM ExamResult
ORDER BY Studentname, 
         Rank;
```

![ ](images/SQL/difference-between-rank-and-dense_rank.png)

![ ](images/SQL/difference-between-rank-and-dense_rank-functio.png)


`NTILE` – функция Transact-SQL, которая делит результирующий набор на группы по определенному столбцу. 

Например:

```sql
SELECT *, 
       NTILE(2) OVER(
       ORDER BY Marks DESC) Rank
FROM ExamResult
ORDER BY rank;
```

Результат:

![ ](images/SQL/ntilen-sql-rank-function.png)

Пример 2:

```sql
SELECT *, 
       NTILE(3) OVER(
       ORDER BY Marks DESC) Rank
FROM ExamResult
ORDER BY rank;
```

Результат:

![ ](images/SQL/ntilen-function-with-partition.png)

Пример 3:

```sql
SELECT *, 
       NTILE(2) OVER(PARTITION  BY subject ORDER BY Marks DESC) Rank
FROM ExamResult
ORDER BY subject, rank;
```

Результат:

![ ](images/SQL/output-of-ntilen-function-with-partition.png)

[к оглавлению](#sql)

## Для чего используются операторы `INTERSECT`, `EXCEPT` в Transact-SQL?
Оператор `EXCEPT` возвращает уникальные записи из левого входного запроса, которые не выводятся правым входным запросом.

Оператор `INTERSECT` возвращает уникальные записи, выводимые левым и правым входными запросами.

[к оглавлению](#sql)

## Напишите запрос...
```sql
CREATE TABLE table ( 
  id BIGINT(20) NOT NULL AUTO_INCREMENT, 
  created TIMESTAMP NOT NULL DEFAULT 0,
  PRIMARY KEY (id) 
);
```

Требуется написать запрос, который вернет максимальное значение `id` и значение `created` для этого `id`:

```sql
SELECT id, created FROM table where id = (SELECT MAX(id) FROM table);
```

---

```sql
CREATE TABLE track_downloads ( 
  download_id BIGINT(20) NOT NULL AUTO_INCREMENT, 
  track_id INT NOT NULL, 
  user_id BIGINT(20) NOT NULL, 
  download_time TIMESTAMP NOT NULL DEFAULT 0, 
  PRIMARY KEY (download_id) 
);
```

Напишите SQL-запрос, возвращающий все пары `(download_count, user_count)`, удовлетворяющие следующему условию: `user_count` — общее ненулевое число пользователей, сделавших ровно `download_count` скачиваний `19 ноября 2010 года`:

```sql
SELECT DISTINCT download_count, COUNT(*) AS user_count 
FROM ( 
    SELECT COUNT(*) AS download_count  
    FROM track_downloads WHERE download_time="2010-11-19" 
    GROUP BY user_id)  
AS download_count
GROUP BY download_count; 
```

[к оглавлению](#sql)

# Источники
+ [Википедия](https://ru.wikipedia.org/wiki/SQL)
+ [Quizful](http://www.quizful.net/interview/sql)

[Вопросы для собеседования](README.md)
Отлично — сделаю развёрнутый «скрипт» для собеса по масштабированию PostgreSQL: перечислю **основные стратегии** (вертикальное масштабирование, пуллинг подключений, репликация/read-replicas, партиционирование, шардинг/scale-out, FDW/ручное шардингование, кэширование и оптимизации запросов), для каждой приведу подробные объяснения, практические примеры (команды/DDL/конфиги), тонкости и подводные камни, плюс блок «частые вопросы на интервью» с готовыми ответами. Там где важно — дам ссылки на официальную документацию/авторитетные источники.

# 1) Вертикальное масштабирование (scale-up)
Определение: Вертикальное масштабирование — это увеличение ресурсов одного сервера (CPU, RAM, дисковая подсистема, сеть), на котором работает PostgreSQL, чтобы повысить его производительность. Это не изменение архитектуры, а «усиление» существующего узла. Подходит для начального этапа роста нагрузки, так как не требует изменений в приложении и инфраструктуре.

Кратко: это «покупаем больше железа» — больше CPU, RAM, NVMe, лучше дисковые подсистемы. Простая стратегия для большинства OLTP-баз на ранних стадиях.

Подробности:
- Почему работает: PostgreSQL — монолитный процесс, многие запросы выигрывают от большего объёма RAM (shared_buffers, OS cache), большего CPU (параллелизм) и быстрых I/O. Простота: без изменений в приложении. Главное — есть пределы: узким местом станет single-node IO/CPU/lock contention.  
- Настройки, которые обычно меняют при вертикальном росте: `shared_buffers` (~25% RAM, но тестируйте), `work_mem`, `maintenance_work_mem`, `max_wal_size`, `checkpoint_timeout`, `effective_cache_size`. Пример:  
  ```conf
  shared_buffers = 16GB
  work_mem = '16MB'
  maintenance_work_mem = '1GB'
  max_wal_size = '4GB'
  checkpoint_timeout = 15min
  ```
- Практическая операция: на облаке — resize VM/instance (CPU/RAM/IOPS), увеличить диск IOPS/throughput. На bare-metal — добавить RAM/CPU. В managed сервисе — resize с возможным downtime.
- Подводные камни: рост `max_connections` не спасает, каждое соединение «дорогое». SSD важнее, чем RAM, если нагрузка — I/O-bound. Ограничение — один сервер.
- Когда выбрать: на ранних стадиях, без требований к геораспределённости.

# 2) Connection pooling (PgBouncer / сторонние poolers)
Определение: Connection pooling — это механизм, при котором множество клиентских подключений «прикрываются» меньшим количеством реальных подключений к PostgreSQL. Пулы соединений позволяют резко снизить нагрузку на сервер, потому что каждое соединение в PostgreSQL потребляет память и ресурсы планировщика. Пуллер выступает посредником между приложением и сервером БД, распределяя запросы по уже открытым соединениям.

Кратко: уменьшает число реальных соединений к Postgres, снижает потребление памяти, ускоряет отклик под нагрузкой.

Подробности:
- PgBouncer держит ограниченное число серверных соединений и переиспользует их. Одно соединение PostgreSQL ≈ 5–20 MB RAM. PgBouncer самый популярный.
- Режимы: `session`, `transaction`, `statement`. Ограничения: нельзя хранить состояние в сессии при `transaction`/`statement`.
- Пример `pgbouncer.ini`:
  ```ini
  [databases]
  myapp = host=10.0.0.5 port=5432 dbname=mydb

  [pgbouncer]
  listen_addr = 0.0.0.0
  listen_port = 6432
  auth_type = md5
  auth_file = /etc/pgbouncer/userlist.txt
  pool_mode = transaction
  max_client_conn = 2000
  default_pool_size = 100
  ```
- Подводные камни: серверные prepared statements, temp tables. Решение: отключать или использовать `session`.
- Когда выбрать: практически всегда при росте нагрузки.

# 3) Репликация и чтение с реплик (read replicas)
Кратко: разделение нагрузки — записи идут на мастер, чтения можно отдавать с реплик.

Подробности:
- Встроенная стриминг-репликация. Команды: `pg_basebackup`, `primary_conninfo`, `standby.signal`.
- Пример `postgresql.conf` на реплике:
  ```conf
  primary_conninfo = 'host=10.0.0.1 port=5432 user=replicator password=xxx'
  hot_standby = on
  ```
- Пример на master:
  ```conf
  wal_level = replica
  max_wal_senders = 10
  wal_keep_size = 1GB
  ```
- Подводные камни: реплики отстают. Для strong-consistency нужен synchronous replication (медленнее).
- Когда выбрать: когда чтений в разы больше, чем записей.

# 4) Партиционирование (partitioning)
**Определение:** Партиционирование — это метод организации данных в одной таблице таким образом, что физически данные хранятся в нескольких подтаблицах (partitions), каждая из которых содержит подмножество строк основной таблицы. Сама таблица становится «контейнером», а PostgreSQL автоматически направляет запросы в нужные разделы в зависимости от условия. Таким образом, пользователю таблица выглядит единой, но реально обращение идёт к меньшему объёму данных.

Кратко: деление одной таблицы на логические куски (partition'ы), управляемые планировщиком.

Подробности:
- В PostgreSQL есть **declarative partitioning** с 10 версии. Типы: `RANGE`, `LIST`, `HASH`. Партиционирование полезно, когда таблица растёт до сотен миллионов и миллиардов строк, и работа с ней становится медленной.
- Пример: создадим таблицу заказов, разделённых по годам.
  ```sql
  CREATE TABLE orders (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT NOT NULL,
    created_at DATE NOT NULL
  ) PARTITION BY RANGE (created_at);

  CREATE TABLE orders_2022 PARTITION OF orders
    FOR VALUES FROM ('2022-01-01') TO ('2023-01-01');

  CREATE TABLE orders_2023 PARTITION OF orders
    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');
  ```
- Что даёт: оптимизатор понимает, в какой partition идти, и делает *partition pruning*. Запрос `WHERE created_at BETWEEN '2022-03-01' AND '2022-03-31'` пойдёт только в `orders_2022`. Это резко сокращает сканирование и ускоряет запросы.
- Плюсы: упрощает администрирование (можно дропать старые партиции), ускоряет запросы по диапазонам. Минусы: сложнее поддерживать индексы и foreign keys, слишком много партиций замедляют планировщик.
- Когда использовать: большие таблицы с естественной сегментацией (например, временные данные, географическая сегментация).
- Частый вопрос: чем отличается партиционирование от шардинга? Ответ — партиционирование внутри одной базы и одного сервера, всё под одним планировщиком. Шардинг — разделение данных по разным физическим серверам.

# 5) Шардирование (scale-out)
**Определение:** Шардирование — это горизонтальное масштабирование базы данных путём разбиения набора данных на отдельные сегменты (шарды), каждый из которых хранится на отдельном физическом сервере или кластере. Каждый шард содержит часть данных, а все шарды вместе представляют собой полную базу. Обычно используется ключ шардинга (shard key), который определяет, к какому серверу отправлять запрос. В отличие от партиционирования, шардинг предполагает распределение нагрузки и данных на несколько независимых узлов.

Кратко: данные распределяются по разным серверам (шардам). Каждая нода хранит часть данных.

Подробности:
- Шардирование нужно, когда один сервер не тянет нагрузку и вертикальный рост уже не помогает. Это следующий шаг после партиционирования.
- Варианты реализации:
  - **Citus** (расширение Postgres): автоматическое распределение по шардам.
  - **Ручное шардингование**: приложение решает, куда писать/читать.
- Пример с Citus:
  ```sql
  CREATE EXTENSION citus;
  CREATE TABLE events (
    id BIGINT,
    user_id BIGINT,
    payload JSONB
  );
  SELECT create_distributed_table('events', 'user_id');
  ```
- Пример ручного шардирования: создаём две базы `orders_shard1`, `orders_shard2`.
  ```sql
  -- shard1
  CREATE TABLE orders (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT,
    amount NUMERIC
  );

  -- shard2
  CREATE TABLE orders (
    id BIGSERIAL PRIMARY KEY,
    user_id BIGINT,
    amount NUMERIC
  );
  ```
  В приложении: `order.user_id % 2 == 0 ? shard1 : shard2`.
- Плюсы: можно практически бесконечно масштабировать нагрузку по горизонтали, каждая нода обрабатывает свою часть данных. Минусы: сложные JOIN’ы по шардам, ограниченные транзакции, сложнее администрирование.
- Когда использовать: petabyte-scale данные, когда таблицы уже не помещаются на один сервер, а нагрузка по CPU/I/O превышает возможности железа.
- Частые вопросы: как решать проблему глобальных JOIN’ов? Ответ: денормализация данных, кэширование агрегатов, использование Citus или FDW.
- Подводный камень: нужно заранее выбрать ключ шардинга. Плохой выбор приведёт к «горячим» шардам и неравномерной нагрузке.


# 6) Foreign Data Wrappers (FDW)
**Определение:** Foreign Data Wrappers (FDW) — это механизм PostgreSQL, позволяющий обращаться к внешним источникам данных (другие PostgreSQL, MySQL, MongoDB, CSV-файлы и т.д.) как к обычным таблицам внутри текущей базы. Они реализуются через расширения, которые создают foreign server и foreign tables. Таким образом, Postgres может выполнять federated queries, объединяя данные из разных систем.

Кратко: Postgres может подключаться к другим Postgres и делать запросы «как будто это одна БД».

Подробности:
- Пример:
  ```sql
  CREATE EXTENSION postgres_fdw;
  CREATE SERVER shard1 FOREIGN DATA WRAPPER postgres_fdw OPTIONS (host '10.0.0.10', dbname 'orders');
  CREATE USER MAPPING FOR CURRENT_USER SERVER shard1 OPTIONS (user 'pguser', password 'pgpass');

  CREATE FOREIGN TABLE orders_shard1 (
    id BIGINT, user_id BIGINT, amount NUMERIC
  ) SERVER shard1 OPTIONS (schema_name 'public', table_name 'orders');
  ```
- Теперь можно делать `SELECT * FROM orders_shard1`. Можно объединять `UNION ALL`.
- Подводные камни: FDW тянет данные постфактум, join pushdown ограничен.
- Когда выбрать: для интеграции разрозненных данных, для прототипов.

  Практические советы по выбору стратегии (когда что выбрать)

Если вы только начинаете или проект небольшой → вертикальное масштабирование + пуллер (PgBouncer) + query tuning.

Если проблема — слишком много соединений/latency на создании соединений → PgBouncer (transaction mode) и фикс конфигов драйверов. 
pgbouncer.org

Если нужно offload read-heavy нагрузки → реплики (streaming), но помните про lag; для строгой консистентности — синхронная репликация (ценой latency). 
PostgreSQL

Если таблицы огромные, но всё в одной ноде помещается — партиционирование (range/hash) + индексы. 
PostgreSQL

Если записи/датасет превышают возможности single node — подумайте о шардинге; если вы не хотите писать собственную логики — Citus (или managed) даёт готовую модель. 
docs.citusdata.com

Частые вопросы на собесе (и короткие правильные ответы — выучите формулировки)

Q1: Чем отличается партиционирование от шардинга?
A: Партиционирование — разделение таблицы внутри одного сервера (RANGE/LIST/HASH) для производительности и управления; шардинг — горизонтальное распределение данных по разным серверам/нодам (scale-out). Партиционирование помогает уменьшить сканирование, шардинг решает проблему single-node capacity. 
PostgreSQL
PlanetScale

Q2: Как настроить реплику и что важно при настройке?
A: Включить wal_level = replica, задать max_wal_senders, создать replication user, сделать pg_basebackup на слейв и задать primary_conninfo/создать standby.signal. Важно: следить за lag, решить вопрос sync/async, настроить failover (Patroni/repmgr). 
PostgreSQL

Q3: Что будет с подготовленными выражениями при PgBouncer в transaction pooling?
A: В transaction и statement режимах серверные prepared statements неперсистентны между транзакциями, поэтому драйверы должны отключать server-side prepares (например, JDBC prepareThreshold=0) или использовать session режим. 
pgbouncer.org

Q4: Как обеспечить уникальность across shards?
A: Нативно — нельзя легко; варианты: (1) включать shard key в состав уникального индекса, (2) централизованный sequence/UUIDs, (3) двухфазные транзакции/координация (дорого). На собесе скажите: «лучше проектировать модель так, чтобы уникальность локальна на шарде».

Q5: Можно ли делать join между шардами?
A: Да, но это дорого: либо приложение подтягивает данные и делает join, либо FDW/Citus маршрутизирует, но меж-шардовые JOIN требуют сетевых вызовов и data movement. Для больших join — лучше репроектировать модель.

Q6: Что такое hot_standby_feedback и зачем он?
A: Параметр, чтобы уменьшить chance of query conflicts на реплике; но он может препятствовать удаления старых транзакций на primary (влияние на bloat). Вопросы такого рода часто задают, поэтому упомяните компромиссы.

Q7: Как мигрировать крупную таблицу на партиционирование без даунтайма?
A: Варианты: создать новую партиционированную таблицу и применить логическую репликацию/ETL, дроп-ин/перенос пачками, использовать pg_repack / триггеры для синхронизации. На собесе объясните пошагово: create partitioned table → sync inserts/updates via trigger/CDC → backfill (batch) → switch over.



# Ответ (подробно, с примерами и комментариями)  
Ниже — развёрнутые ответы на все пункты: объяснения (каждое — **минимум 5 предложений**), ASCII-схемы таблиц для наглядности, SQL-/psql-транзакции с комментариями и Java-практики (что делать на уровне приложения). Я опирался на специфику PostgreSQL (MVCC, реализация уровней изоляции, Serializable Snapshot Isolation — SSI) и даю ссылки на официальную документацию и важные источники там, где это критично.

---

# Введение — кратко о модели PostgreSQL (MVCC)  
PostgreSQL использует MVCC (многоверсионность), т.е. каждая читающая операция видит «снимок» базы в некоторый момент времени, а записи создают новые версионированные кортежи (tuple versions). MVCC даёт важные гарантии: чтение почти никогда не блокирует запись, и наоборот, что улучшает параллельность и производительность. Однако из-за MVCC поведение классических аномалий (dirty read, non-repeatable read, phantom, lost update и т.д.) зависит от конкретного уровня изоляции и от того, реализует ли СУБД сериализуемую изоляцию через блокировки или через обнаружение конфликтов (Postgres использует SSI для SERIALIZABLE). Изменение уровня изоляции в транзакции нужно делать *до* первого запроса в транзакции — иначе SET TRANSACTION не сработает.

---

# 1. Проблемы параллельного доступа (аномалии)
Для каждой аномалии: определение (≥5 предложений), ASCII-таблица, пример транзакций и объяснение в терминологии PostgreSQL.

---

## 1.1 Потерянное обновление (Lost update) — определение
1. Потерянное обновление возникает, когда две или более транзакций читают одно и то же значение, независимо модифицируют его локально, а затем записывают — в результате одна из модификаций «перетирает» другую и данные теряются.  
2. Это происходит потому, что в момент чтения транзакции видят одно и то же состояние, а конечный `UPDATE` не учитывает параллельные изменения, если не применяется блокировка или проверка версии.  
3. Потерянное обновление — это классическая аннотация «write–write» (ww) при неадекватной координации.  
4. В разных СУБД поведение для этой аномалии разнится: одни предотвращают её блокировками, другие — обнаруживают конфликт при коммите.  
5. В PostgreSQL поведение зависит от уровня изоляции и механизмов: например, при `REPEATABLE READ`/`SERIALIZABLE` Postgres обнаруживает конфликты записей и может вызвать ошибку/откат, тогда как при `READ COMMITTED` оно чаще встречается, если не использовать явные блокировки или паттерн optimistic-locking. 

### Таблица (исходное состояние)
```
accounts
+----+---------+---------+
| id | owner   | balance |
+----+---------+---------+
| 1  | alice   | 1000    |
+----+---------+---------+
```

### Сценарий (псевдопараллельно, шаги T1 и T2)
SQL (psql) — последовательность действий:

```sql
-- Подготовка: создаём таблицу
CREATE TABLE accounts (
  id serial PRIMARY KEY,
  owner text,
  balance integer,
  version integer DEFAULT 1  -- версия для optimistic locking (опционально)
);

INSERT INTO accounts (owner, balance) VALUES ('alice', 1000);
```

Сценарий **в Read Committed** (потерянное обновление — возможное):

```sql
-- Транзакция T1: увеличить баланс на 100
-- (соединение A)
BEGIN;                                          -- T1 START
SET TRANSACTION ISOLATION LEVEL READ COMMITTED; -- по умолчанию в Postgres
SELECT balance FROM accounts WHERE id = 1;      -- читает 1000  (snapshot для запроса)
-- ... локальная логика: new = 1000 + 100
UPDATE accounts SET balance = 1100 WHERE id = 1; 
COMMIT;                                         -- T1 COMMIT

-- Транзакция T2: запускается параллельно, читает старое значение до T1 коммита либо после, 
-- но если порядок другой, возможна потеря:
-- (соединение B)
BEGIN;                                         -- T2 START
SET TRANSACTION ISOLATION LEVEL READ COMMITTED; 
SELECT balance FROM accounts WHERE id = 1;     -- может прочитать 1000 (если стартовала до commit T1)
-- ... new = 1000 + 200
UPDATE accounts SET balance = 1200 WHERE id = 1; 
COMMIT;                                        -- T2 COMMIT
```

**Результат:** если T1 и T2 читали 1000 и оба сделали `UPDATE` — последний коммит перетёр первый → потеря обновления.

**Комментарий:** в Postgres при `READ COMMITTED` такое поведение возможно, если не использовать `SELECT ... FOR UPDATE` или optimistic locking. Рекомендуется либо брать пессимистическую блокировку `SELECT ... FOR UPDATE`, либо использовать проверку версии (WHERE version = X) и отслеживать количество обновлённых строк для обнаружения конфликтов. 

**Как предотвратить (Postgres):**
- Пессимистично: `SELECT ... FOR UPDATE` перед изменением (забирает блокировку на строку).  
- Оптимистично: добавление `version`/`xmin` и `UPDATE ... WHERE version = :old_version` + проверка `rows_affected == 1`.  
- Или повышенный уровень изоляции — `REPEATABLE READ`/`SERIALIZABLE` (Postgres выявляет ww-конфликты и может откатывать одну транзакцию). 

---

## 1.2 «Грязное» чтение (Dirty read) — определение
1. «Грязное» чтение — это когда транзакция читает данные, которые были изменены другой транзакцией, но эта другая транзакция ещё не зафиксировала (не выполнила COMMIT).  
2. Если транзакция, чьи изменения были прочитаны, затем сделает ROLLBACK, первая транзакция опиралась на несуществующие в хранилище данные — это нарушение консистентности.  
3. Dirty read — самая «слабая» аномалия, которую защищают уже простые уровни изоляции.  
4. В PostgreSQL уровень `READ UNCOMMITTED` фактически **трактуется как** `READ COMMITTED`: то есть «грязные» чтения **в Postgres невозможны** — СУБД не допускает чтение незакоммиченных изменений.  
5. Следовательно, явная защита от dirty reads в PostgreSQL встроена — вы не сможете прочитать незакоммиченные данные другого сеанса. 

### Демонстрация (попытка грязного чтения — не получится)
```sql
-- Соединение A (T1)
BEGIN;
UPDATE accounts SET balance = 500 WHERE id = 1; -- изменили, но не COMMIT

-- Соединение B (T2)
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED; -- в Postgres это treated as READ COMMITTED
SELECT balance FROM accounts WHERE id = 1;         -- НЕ увидит 500, увидит предыдущее значение
COMMIT;
```

**Комментарий:** попытка прочитать незакоммиченный `UPDATE` не даст «грязного чтения» — Postgres обеспечит, что `SELECT` в другом сеансе увидит только данные, закоммиченные до начала этого запроса. Именно такое поведение описано в документации. 

---

## 1.3 Неповторяющееся чтение (Non-repeatable read) — определение
1. Неповторяющееся чтение — ситуация, когда транзакция T читает одну и ту же строку дважды и получает разные значения, потому что между двумя чтениями другая транзакция изменила и закоммитила эту строку.  
2. Это означает, что повторный `SELECT` в рамках одной транзакции не соответствует первоначальному результату; приложение не может полагаться на устойчивость прочитанных данных.  
3. Неповторяемое чтение обычно предотвращается более строгими уровнями изоляции, которые дают транзакции «стабильный снимок».  
4. В PostgreSQL `READ COMMITTED` **не даёт** повторяемого вида для всей транзакции — каждый `SELECT` может увидеть изменения, которые были закоммичены до начала самого запроса.  
5. В `REPEATABLE READ` и `SERIALIZABLE` транзакция видит один и тот же snapshot на протяжении всей транзакции, поэтому неповторяющееся чтение исключается; при этом в PostgreSQL `REPEATABLE READ` реализован через snapshot isolation. 

### Таблица (для демонстрации)
```
products
+----+---------+-------+
| id | name    | price |
+----+---------+-------+
| 1  | widget  | 100   |
+----+---------+-------+
```

### Сценарий (в Read Committed — неповторяющееся чтение возможно)
```sql
-- Соединение A (T1)
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
SELECT price FROM products WHERE id = 1;  -- увидит 100

-- Соединение B (T2)
BEGIN;
UPDATE products SET price = 120 WHERE id = 1;
COMMIT;  -- изменение зафиксировано

-- Соединение A (T1) снова:
SELECT price FROM products WHERE id = 1;  -- теперь увидит 120 (отличие от первого чтения)
COMMIT;
```

**Комментарий:** в `READ COMMITTED` каждый запрос видит данные, завершённые на момент выполнения **самого запроса**, поэтому второе чтение может отличаться. В `REPEATABLE READ` того же сеанса второй `SELECT` увидит старую версию (100). Для гарантии повторяемости используйте `REPEATABLE READ` или `SERIALIZABLE`. 

---

## 1.4 Чтение «фантомов» (Phantom reads) — определение
1. Фантом-чтение возникает, когда транзакция выполняет запрос (например `SELECT * WHERE cond`) и получает один набор строк, а затем спустя какое-то время повторный тот же запрос возвращает другой набор строк, потому что другая транзакция вставила/удалила строки, подходящие под условие.  
2. Это отличается от неповторяющихся чтений тем, что изменяется **множество строк** (например, увеличилось/уменьшилось количество строк), а не только значение одного поля в конкретной строке.  
3. Классически уровень `REPEATABLE READ` по SQL-стандарту может допускать фантомы; только `SERIALIZABLE` должен предотвращать фантомы.  
4. Но в PostgreSQL `REPEATABLE READ` реализован как Snapshot Isolation (SI), и он *даёт устойчивый снимок на всю транзакцию*, поэтому в Postgres оба уровеня — `REPEATABLE READ` и `SERIALIZABLE` — не позволят увидеть новые строки, вставленные после начала транзакции (т.е. фантомы при повторных запросах не будут видны).  
5. Разница между `REPEATABLE READ` и `SERIALIZABLE` в Postgres заключается не в фантомах (они оба дают устойчивый снимок), а в том, что `SERIALIZABLE` дополнительно выявляет и предотвращает некоторые аномалии сериализации (например, *write skew*) с помощью SSI — и может принудительно откатывать транзакции при обнаружении конфликта. 

### Таблица (для фантома)
```
orders
+----+--------+--------+
| id | status | amount |
+----+--------+--------+
| 1  | open   | 100    |
+----+--------+--------+
```

### Сценарий (повторный SELECT возвращает «фантомы» в Read Committed, но не в Repeatable Read)
```sql
-- Соединение A (T1)
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
SELECT count(*) FROM orders WHERE status = 'open'; -- вернёт 1

-- Соединение B (T2)
BEGIN;
INSERT INTO orders (status, amount) VALUES ('open', 200);
COMMIT;  -- теперь в базе 2 open-заказа

-- Соединение A (T1)
SELECT count(*) FROM orders WHERE status = 'open'; -- в READ COMMITTED вернёт 2 (фантом)
COMMIT;
```

В `REPEATABLE READ` аналогичные два `SELECT` вернули бы 1 и 1 — потому что T1 работает на одном snapshot, он не увидит вставку T2. Если поведение транзакции должно быть полностью сериализуемым с точки зрения корректности бизнес-правил (например, проверки агрегатов, invariants), лучше использовать `SERIALIZABLE` и обработать возможные `serialization_failure` с повтором транзакции.

---

# 2. Уровни изоляции в PostgreSQL — детально (каждый — ≥5 предложений)
PostgreSQL поддерживает четыре SQL-уровня, но реализация и семантика важны.

---

## 2.1 READ UNCOMMITTED (чтение незакоммиченных данных) — подробности
1. SQL-стандарт определяет `READ UNCOMMITTED` как уровень, позволяющий грязные чтения (dirty reads) — т.е. читать незакоммиченные изменения других транзакций.  
2. Однако PostgreSQL **не поддерживает** настоящие грязные чтения: когда вы указываете `SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED`, Postgres фактически выполняет `READ COMMITTED`.  
3. Следовательно, в PostgreSQL `READ UNCOMMITTED` не даст вам повышения параллельности за счёт допуска грязного чтения — это просто алиас для `READ COMMITTED`.  
4. Практический вывод: не рассчитывайте на `READ UNCOMMITTED` в Postgres для каких-то особых эффектов — он просто не позволяет грязных чтений.  
5. Если вы где-то читаете пример с «read uncommitted» — проверьте, к какой СУБД он относится; поведение разнится между Oracle, SQL Server, MySQL и Postgres. 

**Как в SQL:**
```sql
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ UNCOMMITTED;  -- в Postgres трактуется как READ COMMITTED
-- Далее запросы...
COMMIT;
```

---

## 2.2 READ COMMITTED (чтение фиксированных данных) — подробности
1. `READ COMMITTED` — уровень изоляции по умолчанию в PostgreSQL. В этом режиме каждый отдельный SQL-запрос видит только те данные, которые были закоммичены до начала **этого конкретного запроса**.  
2. Это означает, что в одной транзакции два последовательных `SELECT` могут вернуть разные результаты, если между ними другие транзакции закоммитили изменения (т.е. возможны non-repeatable reads и фантомы).  
3. `READ COMMITTED` запрещает грязные чтения — вы не увидите незакоммиченные изменения другого транзакционного сеанса.  
4. Для многих приложений этот уровень — хороший компромисс: высокая параллельность и адекватная консистентность на уровне отдельных запросов.  
5. Если приложение зависит от инвариантов, проверяемых в нескольких запросах в одной транзакции, `READ COMMITTED` обычно недостаточен — в таких случаях следует использовать `REPEATABLE READ` или `SERIALIZABLE`, либо явные блокировки. 

**Пример установки (psql / JDBC):**
```sql
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ COMMITTED; -- default
-- затем запросы
COMMIT;
```
В JDBC можно использовать `connection.setTransactionIsolation(Connection.TRANSACTION_READ_COMMITTED);` до `connection.setAutoCommit(false)`.

---

## 2.3 REPEATABLE READ (повторяющееся чтение) — подробности
1. В PostgreSQL `REPEATABLE READ` реализован как Snapshot Isolation: транзакция получает один и тот же стабильный снимок данных на весь её срок жизни, т.е. все `SELECT` в транзакции видят одно и то же состояние.  
2. Это устраняет non-repeatable reads и фантомы в смысле видимости новых строк — вы не увидите изменения, сделанные и закоммиченные другими транзакциями после старта вашей транзакции.  
3. Однако Snapshot Isolation по своей природе **не является полной сериализуемостью** — существуют специфические аномалии сериализации (например, write-skew), которые могут случиться в SI, но не в истинно сериализуемом исполнении.  
4. PostgreSQL дополнительно способен обнаруживать некоторые конфликтные ситуации и при обнаружении конфликта выдаёт соответствующую ошибку и просит повторить транзакцию (особенно в SERIALIZABLE). Для REPEATABLE READ Postgres предотвращает потерю записей (многие ww-конфликты), но не все общие случаи сериализации, поэтому внимательно проектируйте бизнес-правила.  
5. Если вам нужна абсолютная гарантия сериализуемости (поведение, эквивалентное некоторой последовательной упорядоченности транзакций), используйте `SERIALIZABLE` — Postgres применяет Serializable Snapshot Isolation (SSI) и сможет откатывать транзакции при обнаружении небезопасных конфликтов. 

**Как включить:**
```sql
BEGIN;
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
-- операции
COMMIT;
```

**Примечание для Java-разработчика:** при использовании ORM (например, Hibernate) `@Version` (оптимистическая блокировка) часто применяется вместе с Repeatable Read / Serializable как более явный способ обработки конкурирующих обновлений.

---

## 2.4 SERIALIZABLE (упорядочиваемость) — подробности
1. `SERIALIZABLE` — самый строгий уровень изоляции по SQL-стандарту; он требует, чтобы эффект параллельного выполнения транзакций был эквивалентен какому-то последовательному порядку исполнения.  
2. PostgreSQL реализует `SERIALIZABLE` через механизм Serializable Snapshot Isolation (SSI), который использует обнаружение конфликтов (оптимистический подход) и при необходимости откатывает транзакции, чтобы обеспечить сериализуемость.  
3. В практическом плане при высококонкурентной нагрузке транзакции под `SERIALIZABLE` могут чаще получать ошибку `serialization_failure` и должны быть повторены приложением; поэтому приложения должны быть готовы к автоматическим повторам/ретраям транзакций.  
4. `SERIALIZABLE` предотвращает аномалии, которые SI допускает (например, write-skew), и тем самым даёт сильную гарантию корректности инвариантов уровня приложения без явных блокировок.  
5. Недостаток — возможные откаты и возросшая нагрузка на внутреннее отслеживание взаимосвязей транзакций; в некоторых сценариях достаточно `REPEATABLE READ`, а `SERIALIZABLE` применять только для операций, критичных к целостности. 

**Пример использования и обработка в Java:**
- В JDBC: `connection.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);` или `SET TRANSACTION ISOLATION LEVEL SERIALIZABLE;` в SQL.  
- Всегда окружайте транзакционные операции попытками повторения при перехвате `PSQLException` с SQLState `40001` (`serialization_failure`) и повторяйте транзакцию с экспоненциальной задержкой. 

---

# 3. Практические SQL-примеры (с комментариями) — наглядно

## A) Пример предотвращения потерянного обновления — пессимистический подход (SELECT FOR UPDATE)
```sql
-- подготовка
CREATE TABLE inventory (
  id serial PRIMARY KEY,
  product text,
  qty integer
);
INSERT INTO inventory (product, qty) VALUES ('widget', 10);

-- Соединение A (T1)
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
SELECT qty FROM inventory WHERE product = 'widget' FOR UPDATE;  -- захватываем lock на строку
-- Теперь другие транзакции, пытающиеся взять FOR UPDATE на эту же строку, будут ждать
UPDATE inventory SET qty = qty - 2 WHERE product = 'widget';
COMMIT;

-- Соединение B (T2)
-- пока T1 держит FOR UPDATE, T2 будет ждать, если тоже выполняет FOR UPDATE
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
SELECT qty FROM inventory WHERE product = 'widget' FOR UPDATE; -- заблокируется до COMMIT T1
-- после освобождения T1, T2 продолжит с актуальным value
COMMIT;
```
**Комментарий:** `SELECT ... FOR UPDATE` — типичный пессимистический паттерн; хорошо для операций, где ожидание блокировки приемлемо. 

---

## B) Пример предотвращения потерянного обновления — оптимистический подход (версия / optimistic locking)
```sql
-- подготовка
CREATE TABLE accounts_v (
  id serial PRIMARY KEY,
  owner text,
  balance integer,
  version integer DEFAULT 1
);
INSERT INTO accounts_v (owner, balance) VALUES ('bob', 1000);

-- T1: читаем и пытаемся обновить
BEGIN;
SELECT balance, version FROM accounts_v WHERE id = 1; -- returns balance=1000, version=1
-- приложение вычисляет new_balance = 900
UPDATE accounts_v
SET balance = 900, version = version + 1
WHERE id = 1 AND version = 1;  -- только если версия не изменилась
-- check rows_affected == 1 -> success, иначе приложение должен retry или сообщить конфликт
COMMIT;

-- Если кто-то другой (T2) параллельно уже обновил и поменял version, UPDATE вернёт 0 строк => детект конфликта
```

**Комментарий:** оптимистический вариант удобен для распределённых приложений и хорошо сочетается с retry логикой на клиенте (Java/Hibernate `@Version` реализует это автоматически). 

---

## C) Пример non-repeatable read / phantom (демонстрация с уровнями)
```sql
-- Подготовка
CREATE TABLE orders_demo (id serial PRIMARY KEY, status text, amount int);
INSERT INTO orders_demo (status, amount) VALUES ('open', 100);

-- READ COMMITTED (по умолчанию) — возможно получение разных значений между SELECT-ами
-- T1:
BEGIN;
SET TRANSACTION ISOLATION LEVEL READ COMMITTED;
SELECT count(*) FROM orders_demo WHERE status='open'; -- -> 1

-- T2:
BEGIN;
INSERT INTO orders_demo (status, amount) VALUES ('open', 200);
COMMIT;

-- T1:
SELECT count(*) FROM orders_demo WHERE status='open'; -- -> 2  (неповторяемость / фантом)
COMMIT;

-- REPEATABLE READ:
-- Если в T1 поставить REPEATABLE READ, оба SELECT вернут 1, потому что T1 видит один и тот же snapshot.
```

**Комментарий:** для защиты бизнес-инвариантов, проверяемых несколькими запросами, используйте `SERIALIZABLE` или явные блокировки/инварианты на уровне БД. 

---

## D) Пример сериализуемости и обнаружения аномалий (write-skew) — почему нужен SERIALIZABLE
Сценарий write-skew: две транзакции читают оба условия, каждый обновляет разные строки, но в совокупности нарушается инвариант.

```sql
CREATE TABLE shifts (id serial PRIMARY KEY, emp text, on_call boolean);
INSERT INTO shifts (emp, on_call) VALUES ('alice', true), ('bob', true);

-- Инвариант: по крайней мере один on_call = true всегда должен быть

-- T1:
BEGIN;
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
SELECT emp, on_call FROM shifts;  -- видит alice:true, bob:true
-- Принято решение снять alice
UPDATE shifts SET on_call = false WHERE emp = 'alice';
COMMIT;

-- T2 (параллельно):
BEGIN;
SET TRANSACTION ISOLATION LEVEL REPEATABLE READ;
SELECT emp, on_call FROM shifts;  -- тоже видит alice:true, bob:true
-- Принято решение снять bob
UPDATE shifts SET on_call = false WHERE emp = 'bob';
COMMIT;

-- В результате оба могут успешно закоммититься -> оба on_call = false -> invariant broken.
-- Это классический write-skew, который Snapshot Isolation (REPEATABLE READ) допускает,
-- а SERIALIZABLE в Postgres через SSI обнаружит конфликт и заставит одну из транзакций откатиться.
```

**Комментарий:** если у вас есть глобальные инварианты, которые должны сохраняться, используйте `SERIALIZABLE` и логику повтора транзакций при `serialization_failure`. 

---

# 4. Рекомендации для Java-разработчика (практика на собеседовании)
1. **Знайте семантику уровней изоляции PostgreSQL.** Помните: `READ UNCOMMITTED` → *treated as* `READ COMMITTED` в Postgres; `REPEATABLE READ` ≈ Snapshot Isolation; `SERIALIZABLE` реализован через SSI и может бросать `serialization_failure`. Умение объяснить это — важная часть интервью.  
2. **Практика обработки откатов/повторов:** под `SERIALIZABLE`/иногда под `REPEATABLE READ` нужно предусмотреть повтор транзакции в коде при получении SQLState `40001` (в JDBC это `PSQLException` с `getSQLState() == "40001"`). Паттерн: ограниченное число попыток + экспоненциальный бэк-офф.  
3. **Оптимистическая vs пессимистическая блокировка:** если ожидаются редкие конфликты — используйте optimistic (version column, `UPDATE ... WHERE version = ?`); при высокой конкуренции или критических секциях — используйте `SELECT ... FOR UPDATE` (пессимистично). Hibernate поддерживает `@Version` (оптимист.) и `LockMode.PESSIMISTIC_WRITE`.  
4. **Не полагайтесь на один уровень изоляции для всех сценариев.** Комбинация: бизнес-критичные проверки выполняйте под `SERIALIZABLE` и ретраями, а массовые чтения — под `READ COMMITTED` для производительности. ﹘ бывает разумно ограничивать область транзакции (понижать время удержания locks/snapshots).  
5. **Вопросы на собеседовании, которые могут задать:** объясните MVCC, как Postgres хранит xmin/xmax, чем отличается SI от SERIALIZABLE, приведите пример lost update и способы его предотвращения (FOR UPDATE / versioning), как вы будете повторять транзакции в Java и что такое SQLState `40001`. Подготовьте короткий пример кода JDBC/псевдокод retry logic. 

---

# 5. Полезные короткие примеры Java (JDBC) — retry при SERIALIZABLE
```java
// Псевдо-JDBC: выполнить транзакцию с retry при serialization_failure
int maxAttempts = 5;
for (int attempt = 1; attempt <= maxAttempts; attempt++) {
    try (Connection conn = ds.getConnection()) {
        conn.setAutoCommit(false);
        conn.setTransactionIsolation(Connection.TRANSACTION_SERIALIZABLE);
        // Выполняем бизнес-операции: SELECT, UPDATE и т.д.
        // PreparedStatement ps = conn.prepareStatement(...);
        // ps.executeUpdate();
        conn.commit();
        break; // успех
    } catch (SQLException ex) {
        if ("40001".equals(ex.getSQLState()) && attempt < maxAttempts) {
            // serialization_failure -> retry
            Thread.sleep(100 * attempt); // простая экспоненциальная задержка
            continue;
        } else {
            throw new RuntimeException(ex);
        }
    }
}
```
**Комментарий:** обязательно обрабатывать `40001` и повторять; не пытайтесь внутри одной транзакции менять isolation level после первого запроса — Postgres запрещает.

---

# 6. Краткие ответы/обобщение (на одно дыхание)
- `READ UNCOMMITTED` в PostgreSQL — это не настоящее RU; Postgres трактует его как `READ COMMITTED`.  
- `READ COMMITTED` — видимость на уровне *запроса*; допускает non-repeatable reads и фантомы; предотвращает dirty reads.  
- `REPEATABLE READ` в Postgres — snapshot isolation; устраняет non-repeatable reads и фантомы по видимости, но SI допускает определённые сериализационные аномалии (write-skew).  
- `SERIALIZABLE` — true serializability в Postgres, реализована через SSI; может приводить к откатам транзакций (`serialization_failure`) которые нужно ретраить.  
- Для практики: используйте `SELECT FOR UPDATE` для пессимистического контроля, `version`/optimistic locking для лёгкой и масштабируемой координации, а для критичных инвариантов — `SERIALIZABLE` + retry. 

---


Отлично — ниже полный развёрнутый ответ на русском (подготовлено для собеседования Java-разработчика). Содержит: подробные определения (каждое — минимум 5 предложений), ASCII-таблицы для наглядности, готовый SQL-код (Postgres) с комментариями, примеры для Spring Data / JPA и JDBC, рекомендации, ловушки и практические паттерны. Я привожу также ключевые ссылки на официальную документацию/статьи для самых важных утверждений — потому что в вопросе вы просили «искать во всех источниках» и на собесе это полезно знать. Цитаты стоят рядом с наиболее значимыми утверждениями.

---

# Краткая схема: о чём будет
1. MVCC и видимость строк (строки / tuple header / xmin/xmax) — подробно.  
2. Пессимистическая блокировка (row locks: `FOR UPDATE`, `NOWAIT`, `SKIP LOCKED`, table locks) — определение, примеры SQL, Spring Data / JPA код, когда применять.  
3. Оптимистическая блокировка (версия, `@Version`, `WHERE version = ?`) — определение, примеры SQL, Spring Data код, обработка конфликтов и retry.  
4. Advisory locks (pg_advisory_lock и варианты) — определение, SQL и Java/JdbcTemplate примеры, когда использовать.  
5. Другие модели блокировок и инструменты: таблица lock modes, `LOCK TABLE`, `pg_locks`, deadlocks и их обработка, рекомендации по производительности.  
6. Практические паттерны/рецепты для Java (Spring Data / JDBC / Hibernate) — готовые примеры с обработкой ошибок и retry.  
7. Итог — когда что выбирать, чек-лист для собеседования.

Для наиболее важных технических утверждений (например, «Postgres использует MVCC», «FOR UPDATE блокирует строки», «SERIALIZABLE реализован через SSI», «advisory locks session vs transaction», «SKIP LOCKED для очередей») я оставлю ссылки на официальную документацию / авторитетные материалы прямо в тексте (в конце соответствующих разделов). Это поможет на собеседовании аргументированно отвечать.

---

# 1. MVCC и поведение строк (tuple header, xmin/xmax) — **определение и подробности** (≥5 предложений)
1. PostgreSQL использует MVCC (multiversion concurrency control): вместо блокирования чтений, СУБД хранит несколько версий строк (tuple versions) и каждый запрос/транзакция видит «снимок» базы в некоторый момент времени.  
2. Каждая запись (heap tuple) имеет заголовок с метаданными — в него входят системные поля `xmin` (ID транзакции, которая создала/вставила эту версию) и `xmax` (ID транзакции, которая пометила эту версию как удалённой), а также другие служебные поля; эти значения участвуют в проверке видимости.  
3. При `UPDATE` фактически создаётся новая версия строки (INSERT новой версии) и предыдущая помечается `xmax` = txid обновляющей транзакции; это позволяет параллельным читателям видеть старые версии, а писателям — создать новые.  
4. Видимость строки зависит от снимка транзакции: в `READ COMMITTED` снимок вычисляется для каждого запроса (поэтому повторные SELECT могут видеть новые закоммиченные данные), а в `REPEATABLE READ`/`SERIALIZABLE` транзакция пользуется единственным snapshot’ом на всё время выполнения (поэтому повторные SELECT видят одно и то же).  
5. Из-за MVCC чтения не блокируют записи и записи не блокируют чтения, что повышает параллелизм; однако это порождает необходимость очистки старых версий (VACUUM) и накладывает особенности на модели блокировки при обновлениях (например, потерянные обновления нужно предотвращать на уровне приложения/транзакций).

### Наглядность — ASCII-таблица (исходное состояние)
```
accounts
+----+--------+---------+--------+
| id | owner  | balance | xmin   |
+----+--------+---------+--------+
| 1  | alice  | 1000    | 1000   |
+----+--------+---------+--------+
```
(здесь `xmin` — упрощённый пример: номер транзакции, вставившей кортеж)

---

# 2. Пессимистическая блокировка — объяснение (≥5 предложений)
1. **Пессимистическая блокировка** означает, что транзакция заранее захватывает блокировки на ресурсы (строки или таблицу), чтобы предотвратить параллельные изменения других транзакций; это подход «защищаю ресурс пока работаю».  
2. В PostgreSQL строковые блокировки реализуются через `SELECT ... FOR UPDATE`, `FOR NO KEY UPDATE`, `FOR SHARE`, `FOR KEY SHARE`; эти конструкции заставляют нуждающиеся транзакции ждать, пока блокировка не будет освобождена (или вызывать ошибку, если использовать `NOWAIT`).  
3. `FOR UPDATE` блокирует строки для обновлений/удалений другими транзакциями; `FOR NO KEY UPDATE` похож, но менее строгий для ключевых ограничений; `FOR KEY SHARE` / `FOR SHARE` используются для согласованного чтения при работе с FK и пр. Различия важны при параллельных вставках в дочерние таблицы.  
4. Для очередей и воркеров часто применяют `SKIP LOCKED` (вместе с `FOR UPDATE`) — это позволяет рабочему процессу пропускать уже захваченные строки и взять следующую доступную задачу, уменьшая шанс на deadlock и повышая throughput.  
5. Минусы пессимистики: возможные ожидания/блокировки (latency), вероятность deadlock'ов в случае сложных зависимостей, снижение пропускной способности при высокой конкуренции; плюс — удобна, когда конфликтов много или когда операции критичны и нельзя терять данные.

## 2.1 SQL-пример: базовый `FOR UPDATE`, `NOWAIT`, `SKIP LOCKED`
```sql
-- Подготовка
CREATE TABLE inventory (
  id serial PRIMARY KEY,
  product text UNIQUE,
  qty integer
);

INSERT INTO inventory (product, qty) VALUES ('widget', 10), ('gadget', 5);

-- Транзакция T1: берём строку и обновляем
BEGIN;
SELECT id, qty FROM inventory WHERE product = 'widget' FOR UPDATE; -- захватываем блокировку на строку
-- (выполняем вычисления)
UPDATE inventory SET qty = qty - 1 WHERE product = 'widget';
-- НЕ COMMIT пока не готовы

-- Транзакция T2, запущенная параллельно:
BEGIN;
-- Если T1 держит FOR UPDATE, то обычный SELECT ... FOR UPDATE будет ждать
SELECT id FROM inventory WHERE product = 'widget' FOR UPDATE; -- блокируется до COMMIT T1

-- Используем NOWAIT: если блок занят, запрос вернёт ошибку немедленно
SELECT id FROM inventory WHERE product = 'widget' FOR UPDATE NOWAIT; -- throws error if locked

-- Используем SKIP LOCKED (полезно для воркеров очередей)
SELECT id FROM inventory WHERE qty > 0 FOR UPDATE SKIP LOCKED; 
-- вернёт только те строки, которые не заблокированы, позволяя рабочим параллельно брать разные задачи
```

**Комментарий:** `FOR UPDATE` — явная пессимистическая блокировка строк; `NOWAIT` полезен, если вы хотите сразу обработать ситуацию занятости, `SKIP LOCKED` — для распределённых очередей воркеров.

## 2.2 Spring Data / JPA (пессимистический пример)
```java
// Entity
@Entity
public class Inventory {
    @Id @GeneratedValue Long id;
    @Column(unique=true) String product;
    Integer qty;
    // getters/setters
}

// Repository
public interface InventoryRepository extends JpaRepository<Inventory, Long> {
    @Lock(LockModeType.PESSIMISTIC_WRITE) // эквивалент FOR UPDATE
    @Query("select i from Inventory i where i.product = :product")
    Optional<Inventory> findByProductForUpdate(@Param("product") String product);
}

// Usage in service
@Service
public class InventoryService {
    @Autowired InventoryRepository repo;

    @Transactional
    public void takeOne(String product) {
        Inventory inv = repo.findByProductForUpdate(product)
                           .orElseThrow(() -> new NoSuchElementException());
        if (inv.getQty() <= 0) throw new IllegalStateException("none");
        inv.setQty(inv.getQty() - 1);
        // commit при выходе из транзакции
    }
}
```
**Комментарий:** `@Lock(LockModeType.PESSIMISTIC_WRITE)` заставит JPA выполнить `SELECT ... FOR UPDATE`. В Hibernate есть дополнительные свойства типа `javax.persistence.lock.timeout` для NOWAIT/таймаутов. В production часто используют `SKIP LOCKED` через native query для очередей (см. ниже).

---

# 3. Оптимистическая блокировка — объяснение (≥5 предложений)
1. **Оптимистическая блокировка** основывается на гипотезе «конфликты редки»: транзакция не блокирует ресурс заранее, а при сохранении проверяет, что состояние ресурса не менялось с момента чтения (обычно через колонку `version` или использование системных данных).  
2. В PostgreSQL/ORMах это делается через `version`-столбец (Hibernate `@Version`) — при `UPDATE` выполняется `WHERE id = ? AND version = :oldVersion`; если `rows_affected == 0`, значит произошёл конфликт и приложение должно повторить операцию или сообщить об ошибке.  
3. Оптимистический подход избегает блокировок и повышает параллелизм — он лучше, когда изменения редки и критична общая пропускная способность.  
4. Минусы: если конфликты часты — частые повторы (retries) ведут к большой нагрузке и ухудшению UX; он не предотвращает все виды логических гонок (нужны корректные проверки in-transaction).  
5. Обычно оптимистическая блокировка комбинируется с retry-логикой в приложении (экспоненциальный бэкофф и ограниченное число попыток), и её легко интегрировать с Spring Data / Hibernate (`@Version`) и с «ручным» паттерном `version` + `UPDATE ... WHERE version = ?`.

## 3.1 SQL-пример (ручная версия)
```sql
CREATE TABLE accounts_v (
  id serial PRIMARY KEY,
  owner text,
  balance integer,
  version integer DEFAULT 1
);

INSERT INTO accounts_v (owner, balance) VALUES ('bob', 1000);

-- Клиент (T1) читает:
SELECT id, balance, version FROM accounts_v WHERE id = 1; -- вернёт version = 1

-- Клиент вычисляет new_balance и пытается обновить:
UPDATE accounts_v SET balance = 900, version = version + 1
WHERE id = 1 AND version = 1;

-- Если UPDATE повёрнул rows = 1 — успех. Если 0 — произошло concurrent update, клиент должен retry.
```

## 3.2 Spring Data / JPA (Hibernate `@Version`) — пример
```java
@Entity
public class Account {
    @Id @GeneratedValue Long id;
    String owner;
    Integer balance;
    @Version // Hibernate / JPA will use this column for optimistic locking
    Long version;
    // getters/setters
}

// Репозиторий
public interface AccountRepository extends JpaRepository<Account, Long> {}

// Usage
@Transactional
public void withdraw(Long id, int amount) {
    Account a = repo.findById(id).orElseThrow();
    if (a.getBalance() < amount) throw new IllegalStateException();
    a.setBalance(a.getBalance() - amount);
    // commit -> Hibernate issues UPDATE ... WHERE id=? AND version=?; if row count == 0 -> OptimisticLockException
}
```

**Обработка ошибок и retry (Spring):**
```java
// Пример псевдокода retry
for (int i = 0; i < 5; i++) {
    try {
        myService.withdraw(id, amount);
        break;
    } catch (ObjectOptimisticLockingFailureException ex) {
        // короткая пауза и retry
        Thread.sleep(50 * (i+1));
    }
}
```
**Комментарий:** в Hibernate `OptimisticLockException`/Spring `ObjectOptimisticLockingFailureException` бросается автоматически если версия изменилась. Это надежный и широко используемый подход для многих CRUD-приложений.

---

# 4. Advisory locks — объяснение (≥5 предложений)
1. **Advisory locks** — это «совещательные» блокировки: они не связаны с конкретными строками таблиц, а являются механизмом, который приложение использует для координации доступа к произвольным ресурсам (например, к не-БД ресурсам или к «задаче» в очереди).  
2. PostgreSQL предоставляет набор функций: `pg_advisory_lock`, `pg_try_advisory_lock`, `pg_advisory_xact_lock`, и их shared/try варианты; эти блокировки действуют либо на сессию, либо ограничены транзакцией (xact), в зависимости от вызова.  
3. Отличие: session-уровневый advisory lock держится пока сессия не закроется или пока не вызовут `pg_advisory_unlock`, а transaction-уровневый освобождается автоматически при завершении транзакции; это даёт гибкость при проектировании распределённой синхронизации.  
4. Advisory locks удобны для реализации распределённых семафоров, защиты фоновых задач (only one worker picks job with key X) и управления доступом к внешним ресурсам, но требуют аккуратности — их неправильно освободив, можно заблокировать рабочий поток до завершения сессии.  
5. Важно: advisory locks — это инструмент на уровне приложения; они не защищают сами данные в таблицах от конкурентных обновлений (т.е. нужно соблюдать дисциплину: соглашение между приложениями использовать тот же ключ для блокировки).

## 4.1 SQL-пример advisory locks
```sql
-- Попытаться получить advisory lock (браузер/session level)
SELECT pg_try_advisory_lock(12345); -- вернёт true если удалось, false если нет

-- Получить и ждать (blocking)
SELECT pg_advisory_lock(12345); -- будет ждать пока lock не освободится

-- Посвященный для транзакции (освобождается при COMMIT/ROLLBACK)
SELECT pg_advisory_xact_lock(67890);

-- Снять lock (для session locks)
SELECT pg_advisory_unlock(12345);
```

## 4.2 Java (JdbcTemplate) — advisory_lock пример
```java
Boolean got = jdbcTemplate.queryForObject(
    "SELECT pg_try_advisory_lock(?)", Boolean.class, 12345L);

if (Boolean.TRUE.equals(got)) {
    try {
        // безопасно работать с ресурсом
    } finally {
        jdbcTemplate.update("SELECT pg_advisory_unlock(?)", 12345L);
    }
} else {
    // lock занят — можно retry или пропустить
}
```
**Комментарий:** advisory locks хорошо работают когда нужно координировать задачи вне таблиц, или реализовать «token» для работы с внешними сервисами, но не заменяют транзакционную защиту данных внутри БД.

---

# 5. Другие виды блокировок / системные lock modes / инструменты
**Кратко о lock modes:** есть table-level блокировки (`ACCESS SHARE`, `ROW SHARE`, `ROW EXCLUSIVE`, `SHARE UPDATE EXCLUSIVE`, `SHARE`, `SHARE ROW EXCLUSIVE`, `EXCLUSIVE`, `ACCESS EXCLUSIVE`), подробная матрица конфликтов — в документации. `LOCK TABLE` позволяет вручную захватить table-level lock. Для анализа текущих блокировок есть представление `pg_locks`.

**Deadlocks:** Postgres автоматически обнаруживает дедлоки и завершает одну из транзакций (выдаёт ошибку deadlock detected). Чтобы снизить вероятность дедлоков: придерживайтесь фиксированного порядка захвата ресурсов, держите транзакции короткими, используйте `SKIP LOCKED` для очередей и реже применяйте долгие `FOR UPDATE`.

**Инструменты диагностики:**
- `SELECT * FROM pg_locks JOIN pg_class ON ...` — посмотреть кто чего держит.  
- `EXPLAIN` и `EXPLAIN ANALYZE` — план запросов (для понимания, не напрямую о блокировках).  
- Мониторинг параметров `deadlock_timeout`, `lock_timeout` и `statement_timeout`.

---

# 6. Практические паттерны (рецепты) для Java / Spring — когда и что выбирать

## 6.1 Когда использовать оптимистическую блокировку
- Когда конфликты редки (большинство транзакций не пересекаются по строкам).  
- Типовые CRUD-операции, web-формы, рейтинги, инвентаризация с низкой конкуренцией.  
- Плюсы: нет «ожиданий», высокая параллельность. Минусы: при частых конфликтах — много повторов.  

**Реализация:** Hibernate `@Version` + обработка `ObjectOptimisticLockingFailureException` + retry. Пример кода — выше (см. §3.2).

## 6.2 Когда использовать пессимистическую блокировку
- Когда операция критична и нельзя допустить параллельного изменения (например, перевод денег, резервирование уникального ресурса).  
- Когда ожидается высокая конкуренция на одних и тех же строках и ожидание блокировки предпочтительнее частых повторов.  
- Реализация: `SELECT ... FOR UPDATE` (через `@Lock(LockModeType.PESSIMISTIC_WRITE)`), `NOWAIT` если нужно быстро реагировать на блокировки, `SKIP LOCKED` для очередей.

## 6.3 Использование `SKIP LOCKED` для обработчиков очередей (worker)
```sql
-- пример «получить следующую задачу»
BEGIN;
WITH task AS (
  SELECT id FROM job_queue
  WHERE state = 'pending'
  ORDER BY created_at
  FOR UPDATE SKIP LOCKED
  LIMIT 1
)
UPDATE job_queue q
SET state = 'processing', worker = current_setting('my.worker_id')
FROM task
WHERE q.id = task.id
RETURNING q.*;
-- затем commit
```
**Комментарий:** `SKIP LOCKED` позволяет масштабировать воркеров: если строка уже забронирована— она пропускается, другим воркерам достанутся следующие задачи.

## 6.4 Использование advisory locks для внешних ресурсов и распределённой синхронизации
- Используйте `pg_try_advisory_lock` для неблокирующего запроса; хорош для cron/джобов, которые должны запускаться «в одиночке» в кластере.  
- Для транзакционно-ограниченных операций используйте `pg_advisory_xact_lock` — автоматическое снятие при COMMIT/ROLLBACK.

---

# 7. Примеры «всё в одном» — сценарии и код

## 7.1 Сценарий: «Резервирование товара» — избегаем потерянного обновления (пессимистично)
ASCII-таблица:
```
inventory
+----+---------+-----+
| id | product | qty |
+----+---------+-----+
| 1  | widget  | 5   |
+----+---------+-----+
```

SQL (psql):
```sql
-- Worker A
BEGIN;
SELECT qty FROM inventory WHERE product='widget' FOR UPDATE; -- блокирует строку
-- qty = 5, проверка
UPDATE inventory SET qty = qty - 1 WHERE product = 'widget';
COMMIT;

-- Worker B (параллельно) при попытке:
BEGIN;
SELECT qty FROM inventory WHERE product='widget' FOR UPDATE; -- будет ждать пока A не COMMIT
-- если нужно немедленно узнать — используем FOR UPDATE NOWAIT
```

Spring Data:
```java
@Transactional
public boolean reserve(String product) {
    Optional<Inventory> oi = repo.findByProductForUpdate(product); // PESSIMISTIC_WRITE
    if (!oi.isPresent()) return false;
    Inventory i = oi.get();
    if (i.getQty() <= 0) return false;
    i.setQty(i.getQty() - 1);
    return true;
}
```

## 7.2 Сценарий: «Фоновая очередь с SKIP LOCKED» — предпочтительный вариант для масштаба
SQL:
```sql
BEGIN;
WITH t AS (
  SELECT id FROM jobs WHERE state='pending' FOR UPDATE SKIP LOCKED LIMIT 1
)
UPDATE jobs j SET state='processing' FROM t WHERE j.id = t.id RETURNING j.*;
-- process job
COMMIT;
```
Java (JdbcTemplate):
```java
Map<String,Object> job = jdbcTemplate.queryForMap("WITH t AS (SELECT id FROM jobs WHERE state='pending' FOR UPDATE SKIP LOCKED LIMIT 1) UPDATE jobs j SET state='processing' FROM t WHERE j.id = t.id RETURNING j.*");
```

---

# 8. Инструменты для отладки / мониторинга и важные параметры
- `pg_locks` — посмотреть текущие блокировки.  
- `lock_timeout` — можно настроить поведение ожидания захвата lock (выбрасывать ошибку после таймаута).  
- `deadlock_timeout` — минимальное время до начала проверки дедлока; tuning помогает.  
- Следите за длиной транзакций — длинные транзакции увеличивают число версий и нагрузку на VACUUM.

---

# 9. Частые вопросы на собеседовании (шпаргалка)
- Объясните MVCC и почему чтения не блокируют записи. (см. §1)  
- Чем отличаются `FOR UPDATE`, `FOR NO KEY UPDATE`, `FOR SHARE`, `FOR KEY SHARE`? (FOR UPDATE — жёсткая блокировка для последующих UPDATE/DELETE; NO KEY UPDATE менее строгий; KEY SHARE/SHARE для чтения, взаимодействия с FK).  
- Как реализовать optimistic locking в JPA/Hibernate? (`@Version`). (см. §3)  
- Что такое advisory locks и когда их применять? (см. §4)  
- Чем отличается `REPEATABLE READ` от `SERIALIZABLE` в Postgres? (Postgres: `REPEATABLE READ` = snapshot isolation, `SERIALIZABLE` реализован через SSI; `SERIALIZABLE` может откатить транзакции при обнаружении конфликтов).

---

# 10. Итоги — рекомендации (коротко)
- Если данные редко конфликтуют → используйте **оптимистичную** стратегию (`@Version` / version column).  
- Если вам нужна жёсткая гарантия на отдельную строку, или операции сложно повторить → **пессимистичный** `FOR UPDATE` (с NOWAIT или с таймаутом при необходимости).  
- Для фоновых воркеров и очередей — `FOR UPDATE SKIP LOCKED`.  
- Для межсервисной/внешней синхронизации — рассмотрите **advisory locks** (`pg_try_advisory_lock` / `pg_advisory_xact_lock`).  
- При использовании `SERIALIZABLE` готовьте retry-логику на `serialization_failure` (SQLState `40001`).

---



[Вопросы для собеседования](README.md)
